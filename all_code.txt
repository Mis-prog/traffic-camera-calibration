# ==== example\karls_marks\example.py ====
import numpy as np
import matplotlib.pyplot as plt
from source import CalibrationPipeline, Camera, VanishingPointCalibration, \
    RefineOptimizer, PointND
from source.calibration.utils import load_lines, load_lines_from_json, extract_direction_vectors_from_lines
from source.calibration.refine import residual_interline_distance, residual_line_length, residual_reprojection_line, \
    residual_vertical_alignment, residual_planar_alignment, residual_alignment_block
from source.calibration.debug import load_scene_gps, visualize_source, generate_yandex_maps_url, \
    compute_alignment_and_metrics
from source.vp_detection import VanishingPointEstimatorManual
from source.calibration.utils import gps_to_enu, enu_to_gps
from source.annotation_tools import AnnotationParser
from calibration.debug import load_scene_gps, visualize_source, projection_line
annotation_parser = AnnotationParser("data/data_full_new.json")
lines_vp1 = annotation_parser.get_lines_by_class("vp1")
lines_vp3 = annotation_parser.get_lines_by_class("vp3")
annotation_parser = AnnotationParser("data/data_new.json")
vp1_manual = VanishingPointEstimatorManual().estimate(lines_vp1)
vp3_manual = VanishingPointEstimatorManual().estimate(lines_vp3)
vps_manual = np.array([vp1_manual, vp3_manual])
camera = Camera('data/pattern_corrected_image.png')
vp_init = VanishingPointCalibration(camera, debug_save_path='data/vp.png')
vp_init.set_vanishing_points(vpX=vps_manual[0], vpZ=vps_manual[1])
def back_refine(camera):
    """
    Результататы для перекрестка
    [899.24, -13.76, 48.15, -164.42, 0.0, 0.0, 20.02]
    """
    global annotation_parser
    data = {
        "Пешеходный переход 1": annotation_parser.get_lines_by_class("Пешеходный переход 1"),
        "Пешеходный переход 2": annotation_parser.get_lines_by_class("Пешеходный переход 2"),
        "Пешеходный переход 3": annotation_parser.get_lines_by_class("Пешеходный переход 3"),
        "Дорожные линии 2": annotation_parser.get_lines_by_class("Дорожные линии 2"),
        "Вертикальные линии": annotation_parser.get_lines_by_class("vp3"),
        "Линии в плоскости изображения": annotation_parser.get_lines_by_class("Горизонтальные линии")
    }
    resualds_blocks_first = [
        lambda cam, data: residual_interline_distance(cam, data, group="Пешеходный переход 1", expected=4.2),
        lambda cam, data: residual_interline_distance(cam, data, group="Пешеходный переход 2", expected=4.2),
        lambda cam, data: residual_interline_distance(cam, data, group="Пешеходный переход 3", expected=4.2),
        lambda cam, data: residual_interline_distance(cam, data, group="Дорожные линии 2", expected=3.5),
    ]
    refiner_1 = RefineOptimizer(
        camera=camera,
        residual_blocks=resualds_blocks_first,
        mask=[6],
        bounds=[(5, 30)],
        debug_save_path='data/grid_back_1.png',
        method="minimize",
    )
    refiner_2 = RefineOptimizer(
        camera=camera,
        residual_blocks=resualds_blocks_first,
        mask=[0],
        bounds=[(700, 1500)],
        debug_save_path='data/grid_back_2.png',
        method="minimize",
    )
    resualds_blocks_second = [
        lambda cam, data: residual_alignment_block(
            verticals=extract_direction_vectors_from_lines(data["Вертикальные линии"]),
            planar_lines=extract_direction_vectors_from_lines(data["Линии в плоскости изображения"]),
            weights=(1.0, 1.0, 100.0)
        )(cam, data)
    ]
    refiner_3 = RefineOptimizer(
        camera=camera,
        residual_blocks=resualds_blocks_second,
        omega_mode=True
    )
    pipeline = CalibrationPipeline(
        init_stage=vp_init,
        refine_stages=[refiner_1, refiner_2, refiner_3],
        n_iter=20
    )
    camera = pipeline.run(camera, data)
    return camera
"""
- Проблема с масштабом
- Возможно проблема в углах.
"""
def direct_refine(camera):
    """
    Результаты для перекрестка
    """
    gps_origin = (54.725376, 55.941034)
    data = {
        "all": annotation_parser.get_lines_with_gps_and_pixel("all"),
    }
    print(data)
    residual_blocks_first = [
        lambda cam, data: residual_reprojection_line(cam, data, group="all",
                                                     gps_origin=gps_origin),
    ]
    refiner_1 = RefineOptimizer(
        camera=camera,
        residual_blocks=residual_blocks_first,
        debug_save_path='data/grid_direct_1.png',
        mask=[1],
        bounds=([-360],
                [360]),
        method="trf",
    )
    pipeline = CalibrationPipeline(
        init_stage=vp_init,
        refine_stages=[refiner_1],
        n_iter=1
    )
    camera = pipeline.run(camera, data)
    projection_line(camera, annotation_parser.get_lines_with_gps_and_pixel("all"), 54.725378, 55.941036,
                    save_path='data/projection_line.png')
    return camera
camera = direct_refine(camera)
# ==== example\karls_marks\example_new.py ====
from source import CalibrationPipeline, Camera, VanishingPointCalibration, \
    RefineOptimizer
from source.annotation_tools import load_lines, load_lines_from_json
from calibration.refine import residual_interline_distance, residual_parallel_group, \
    residual_reprojection_line, residual_reprojection_point, residual_line_length
from calibration.debug import load_scene_gps, visualize_source, projection_line
import numpy as np
from source.annotation_tools import AnnotationParser
from calibration.debug import compute_alignment_and_metrics
from source.vp_detection import VanishingPointEstimatorManual
from source.calibration.base import RESUALDS
from calibration.debug import plot_residuals_comparison
# ==== example\prospect\example.py ====
import numpy as np
import matplotlib.pyplot as plt
from source import CalibrationPipeline, Camera, VanishingPointCalibration, \
    RefineOptimizer, PointND
from source.calibration.utils import load_lines, load_lines_from_json, extract_direction_vectors_from_lines
from source.calibration.refine import residual_interline_distance, residual_line_length, residual_reprojection_line, \
    residual_vertical_alignment, residual_planar_alignment, residual_alignment_block, get_plane_normal
from source.calibration.debug import load_scene_gps, visualize_source, generate_yandex_maps_url, \
    compute_alignment_and_metrics
from source.vp_detection import VanishingPointEstimatorManual
from source.calibration.utils import gps_to_enu, enu_to_gps
from source.annotation_tools import AnnotationParser
from calibration.debug import load_scene_gps, visualize_source, projection_line
annotation_parser = AnnotationParser("data/data_full.json")
lines_vp1 = annotation_parser.get_lines_by_class("vp1")
lines_vp3 = annotation_parser.get_lines_by_class("vp3")
vp1_manual = VanishingPointEstimatorManual().estimate(lines_vp1)
vp3_manual = VanishingPointEstimatorManual().estimate(lines_vp3)
camera = Camera('data/pattern_corrected_image.png')
vp_init = VanishingPointCalibration(camera, debug_save_path='data/vp.png')
vp_init.set_vanishing_points(vpX=vp1_manual, vpZ=vp3_manual)
def back_refine(camera, not_init=True):
    global data
    data = {
        "Расстояние между линиями": annotation_parser.get_lines_by_class("vp1"),
    }
    resualds_blocks_first = [
        lambda cam, data: residual_interline_distance(cam, data, group="Расстояние между линиями", expected=3.4),
    ]
    global refiner_1
    refiner_1 = RefineOptimizer(
        camera=camera,
        residual_blocks=resualds_blocks_first,
        mask=[6],
        bounds=[[14], [60]],
        debug_save_path='data/grid_back_1.png',
        method="trf",
        grid_range=(15, 15),
        point_start=[1920 / 2 - 360, 1080 / 2 - 100]
    )
    refiner_2 = RefineOptimizer(
        camera=camera,
        residual_blocks=resualds_blocks_first,
        mask=[0, 6],
        bounds=[[1000, 14], [2000, 60]],
        debug_save_path='data/grid_back_1.png',
        method="trf",
        grid_range=(15, 15),
        point_start=[1920 / 2 + 100, 1080 / 2 + 270]
    )
    pipeline = CalibrationPipeline(
        init_stage=vp_init,
        refine_stages=[refiner_1, refiner_2],
        n_iter=1
    )
    camera = pipeline.run(camera, data)
    return camera
camera = back_refine(camera)
# ==== example\pushkin_aksakov\example.py ====
from source import CalibrationPipeline, Camera, VanishingPointCalibration, \
    RefineOptimizer
from source.annotation_tools import load_lines, load_lines_from_json
from calibration.refine import residual_interline_distance, residual_parallel_group, \
    residual_reprojection_line
from calibration.debug import load_scene_gps, visualize_source, projection_line
import numpy as np
from source.annotation_tools import AnnotationParser
from calibration.debug import compute_alignment_and_metrics
camera = Camera('image/undistorted_output_one.jpg')
annotation_parser = AnnotationParser("marked/data_full.json")
lines_vp1 = annotation_parser.get_lines_by_class("vp1")
lines_vp3 = annotation_parser.get_lines_by_class("vp3")
from source.vp_detection import VanishingPointEstimatorManual
vp1_manual = VanishingPointEstimatorManual().estimate(lines_vp1)
vp3_manual = VanishingPointEstimatorManual().estimate(lines_vp3)
print(vp1_manual, vp3_manual)
vp_init = VanishingPointCalibration(camera, debug_save_path='image/vp.png')
vp_init.set_vanishing_points(vpX=vp1_manual, vpZ=vp3_manual)
annotation_parser = AnnotationParser("marked/data_full.json")
point_control = annotation_parser.get_points_with_gps_and_pixel("Контрольные GPS точки")
point_image, point_gps = [], []
for point in point_control:
    _point_image, _point_gps = point["pixel"], point["gps"]
    point_image.append(_point_image)
    point_gps.append(_point_gps)
def back_refine(camera):
    """
    Параметры камеры [934.15, -158.11, 51.84, 172.31, 0.0, 0.0, 20.89] (не оптимизировал углы)
    """
    data = {
        "Перешеходный переход 1": annotation_parser.get_lines_by_class("Перешеходный переход 1"),
        "Перешеходный переход 2": annotation_parser.get_lines_by_class("Перешеходный переход 2"),
        "Перешеходный переход 3": annotation_parser.get_lines_by_class("Перешеходный переход 3"),
        "Перешеходный переход 4": annotation_parser.get_lines_by_class("Перешеходный переход 4"),
        "Расстояние между линиями": annotation_parser.get_lines_by_class("Расстояние между линиями"),
        "Расстояние между линиями 2": annotation_parser.get_lines_by_class("Расстояние между линиями 2"),
        "Вертикальные линии": annotation_parser.get_lines_by_class("Вертикальные линии"),
        "Горизонтальные линии": annotation_parser.get_lines_by_class("Горизонтальные линии"),
    }
    resualds_blocks_1 = [
        lambda cam, data: residual_interline_distance(cam, data, group="Перешеходный переход 1", expected=4),
        lambda cam, data: residual_interline_distance(cam, data, group="Перешеходный переход 2", expected=4),
        lambda cam, data: residual_interline_distance(cam, data, group="Перешеходный переход 3", expected=4),
        lambda cam, data: residual_interline_distance(cam, data, group="Перешеходный переход 4", expected=4),
        lambda cam, data: residual_interline_distance(cam, data, group="Расстояние между линиями", expected=3.2),
        lambda cam, data: residual_interline_distance(cam, data, group="Расстояние между линиями 2", expected=7),
    ]
    refiner_1 = RefineOptimizer(camera=camera,
                                residual_blocks=resualds_blocks_1,
                                mask=[6],
                                bounds=[[12], [40]],
                                debug_save_path='image/grid_back_1.png',
                                gps_origin=(54.723767, 55.933369),
                                method="trf",
                                grid_range=(10, 10),
                                )
    refiner_2 = RefineOptimizer(camera=camera,
                                residual_blocks=resualds_blocks_1,
                                mask=[0, 6],
                                bounds=[[900, 20], [1700, 40]],
                                debug_save_path='image/grid_back_2.png',
                                gps_origin=(54.723767, 55.933369),
                                method="trf",
                                grid_range=(20, 10),
                                )
    pipeline = CalibrationPipeline(
        init_stage=vp_init,
        refine_stages=[
            refiner_1,
            refiner_2
        ],
        n_iter=1
    )
    camera = pipeline.run(camera, data)
    return camera
def direct_refine():
    """
    Параметры камеры 1167.74, -142.28, 49.46, 172.08, 0.0, 0.0, 30.37 погрешность 65 см
    """
    global camera
    data = {"lines_gps_and_pixel": load_lines_from_json('marked/lines_gps_to_pixel.json'),
            "Размеченные линии": annotation_parser.get_lines_with_gps_and_pixel("Размеченные линии")
            }
    print(data)
    resualds_blocks = [
        lambda cam, data: residual_reprojection_line(cam, data, group="Размеченные линии",
                                                     gps_origin=(54.723617, 55.933152)),
    ]
    refiner_1 = RefineOptimizer(camera=camera,
                                residual_blocks=resualds_blocks,
                                debug_save_path='image/grid_direct_1.png',
                                mask=[0, 1, 2, 3, 6],
                                bounds=([800, -360, -360, -360, 13],
                                        [2000, 360, 360, 360, 35]),
                                method="trf",
                                grid_range=(10, 10)
                                )
    pipeline = CalibrationPipeline(
        init_stage=vp_init,
        refine_stages=[refiner_1],
        n_iter=1,
    )
    camera = pipeline.run(camera, data)
    projection_line(camera, annotation_parser.get_lines_with_gps_and_pixel("Размеченные линии"), 54.723617, 55.933152,
                    save_path='image/projection_line.png')
    return camera
camera = direct_refine()  # Дооптимизация через прямую проекцию
data = compute_alignment_and_metrics(point_image, point_gps, 54.723617, 55.933152, camera, save_path="direct.html")
def gibrid():
    global camera
    data = {
        "dist_between_line_1": load_lines('marked/dist_between_line_1.json'),
        "dist_between_line_2": load_lines('marked/dist_between_line_2.json'),
        "lane_lines": load_lines('marked/parallel_line_1.json'),
        "lines_gps_and_pixel": load_lines_from_json('marked/lines_gps_to_pixel.json')
    }
    resualds_blocks_first = [
        lambda cam, data: residual_interline_distance(cam, data, group="dist_between_line_1", expected=8),
        lambda cam, data: residual_interline_distance(cam, data, group="dist_between_line_2", expected=5.5),
        lambda cam, data: residual_parallel_group(cam, data, group="lane_lines"),
    ]
    refiner_first = RefineOptimizer(camera=camera,
                                    residual_blocks=resualds_blocks_first,
                                    mask=[0, 6],
                                    bounds=([900, 5], [2000, 30]),
                                    debug_save_path='image/')
    resualds_blocks_second = [
        lambda cam, data: residual_reprojection_line(cam, data, group="lines_gps_and_pixel",
                                                     gps_origin=(54.723767, 55.933369)),
    ]
    refiner_second = RefineOptimizer(camera=camera, residual_blocks=resualds_blocks_second, debug_save_path='image/')
    pipeline = CalibrationPipeline([vp_init, refiner_first, refiner_second])
    camera = pipeline.run(camera, data)
from scipy.spatial.transform import Rotation as R
from scipy.optimize import minimize
rz, rx, ry = camera.extrinsics.get_angles()
K = camera.intrinsics.get()
annotation_parser = AnnotationParser("marked/data_angle.json")
lines = annotation_parser.get_lines_by_class("all")
def build_rotation_matrix_zxy(pitch_deg, yaw_deg, roll_deg=0.0):
    return R.from_euler('zxy', [roll_deg, pitch_deg, yaw_deg], degrees=True).as_matrix()
def loss_in_plane_v2(angles_xy_deg, roll_fixed_deg, lines, K, reg_weight=0.1):
    pitch, yaw = angles_xy_deg
    R_new = build_rotation_matrix_zxy(pitch, yaw, roll_fixed_deg)
    n_world = np.array([0, 0, 1])  # предполагаем горизонтальную плоскость
    loss = 0.0
    for p1, p2 in lines:
        ray1 = np.linalg.inv(K) @ np.array([*p1, 1.0])
        ray2 = np.linalg.inv(K) @ np.array([*p2, 1.0])
        line_dir_cam = ray2 - ray1
        line_dir_cam = line_dir_cam / np.linalg.norm(line_dir_cam)
        line_dir_world = R_new.T @ line_dir_cam
        dot_product = np.abs(np.dot(line_dir_world, n_world))
        loss += dot_product ** 2
    reg = reg_weight * np.sum(np.array([pitch, yaw]) ** 2)
    return loss / len(lines) + reg
result = minimize(
    loss_in_plane_v2,
    x0=[rx, ry],
    args=(rz, lines, K),
    method='Powell',
    options={'maxiter': 300, 'disp': True}
)
print(result.x)# ==== example\pushkin_aksakov\example_new.py ====
from source import CalibrationPipeline, Camera, VanishingPointCalibration, \
    RefineOptimizer
from source.annotation_tools import load_lines, load_lines_from_json
from calibration.refine import residual_interline_distance, residual_parallel_group, \
    residual_reprojection_line, residual_reprojection_point, residual_line_length
from calibration.debug import load_scene_gps, visualize_source, projection_line
import numpy as np
from source.annotation_tools import AnnotationParser
from calibration.debug import compute_alignment_and_metrics
from source.vp_detection import VanishingPointEstimatorManual
from source.calibration.base import RESUALDS
from calibration.debug import plot_residuals_comparison
import matplotlib.pyplot as plt
camera = Camera('image/undistorted_output_one.jpg')
annotation_parser = AnnotationParser("marked/data_full_new.json")
lines_vp1 = annotation_parser.get_lines_by_class("vp1")
lines_vp3 = annotation_parser.get_lines_by_class("vp3")
vp1_manual = VanishingPointEstimatorManual().estimate(lines_vp1)
vp3_manual = VanishingPointEstimatorManual().estimate(lines_vp3)
vp_init = VanishingPointCalibration(camera, debug_save_path='image/vp.png')
vp_init.set_vanishing_points(vpX=vp1_manual, vpZ=vp3_manual)
annotation_parser = AnnotationParser("marked/DATA_NEW.json")
def refine(camera):
    data_refine = {
        "Ошибка проекции точек": annotation_parser.get_points_with_gps_and_pixel("Ошибка проекции точек"),
        "Межлинейное расстояние": annotation_parser.get_lines_by_class("Межлинейное расстояние"),
        "Расстояние линий": annotation_parser.get_lines_by_class("Расстояние линий"),
    }
    resualds_blocks = [
        lambda cam, data: (
            np.array(residual_reprojection_point(cam, data, group='Ошибка проекции точек',
                                                 gps_origin=(54.723617, 55.933152))) * (1 / 100),
            'Ошибка проекции точек'
        ),
        lambda cam, data: (
            np.array(residual_line_length(cam, data, group='Расстояние линий', expected=4)) * (1),
            'Расстояние линий'),
        lambda cam, data: (
            np.array(residual_interline_distance(cam, data, group='Межлинейное расстояние', expected=3.25)) * (1),
            'Межлинейное расстояние'),
    ]
    refine = RefineOptimizer(
        camera=camera,
        residual_blocks=resualds_blocks,
        mask=[0, 1, 2, 3, 6],
        bounds=[[500, -360, -360, -360, 10],
                [1500, 360, 360, 360, 40]],
        method='trf',
        debug_save_path='image/grid.png'
    )
    pipeline = CalibrationPipeline(
        init_stage=vp_init,
        refine_stages=[refine],
        n_iter=1,
    )
    pipeline.run(camera=camera, data=data_refine)
    plot_residuals_comparison(RESUALDS)
    return camera
camera = refine(camera)
point_control = annotation_parser.get_points_with_gps_and_pixel("Контрольные точки")
point_image, point_gps = [], []
for point in point_control:
    _point_image, _point_gps = point["pixel"], point["gps"]
    point_image.append(_point_image)
    point_gps.append(_point_gps)
point_control = annotation_parser.get_points_with_gps_and_pixel("Ошибка проекции точек")
for point in point_control:
    _point_image, _point_gps = point["pixel"], point["gps"]
    point_image.append(_point_image)
    point_gps.append(_point_gps)
data = compute_alignment_and_metrics(point_image, point_gps, 54.723617, 55.933152, camera, save_path="direct.html")
# ==== example\report\create_grid.py ====
import cv2
import numpy as np
def create_black_square_grid(cell_size=40, num_rows=6, num_cols=8, margin=40, square_size=20):
    """
    Создаёт изображение с чёрными квадратами, равномерно размещёнными в сетке на белом фоне.
    Parameters:
        cell_size: int — расстояние между центрами квадратов
        num_rows: int — число рядов
        num_cols: int — число колонок
        margin: int — отступ от краёв изображения
        square_size: int — размер стороны чёрного квадрата
    Returns:
        np.ndarray — изображение с сеткой
    """
    height = 2 * margin + cell_size * (num_rows - 1)
    width = 2 * margin + cell_size * (num_cols - 1)
    image = np.ones((height, width, 3), dtype=np.uint8) * 255  # белый фон
    for row in range(num_rows):
        for col in range(num_cols):
            center_x = margin + col * cell_size
            center_y = margin + row * cell_size
            top_left = (center_x - square_size // 2, center_y - square_size // 2)
            bottom_right = (center_x + square_size // 2, center_y + square_size // 2)
            cv2.rectangle(image, top_left, bottom_right, color=(0, 0, 0), thickness=-1)
    return image
import cv2
import numpy as np
def create_line_grid(cell_size=40, num_rows=6, num_cols=8, margin=40, thickness=1):
    """
    Создаёт изображение с горизонтальными и вертикальными линиями, равномерно распределёнными на белом фоне.
    Parameters:
        cell_size: int — расстояние между линиями
        num_rows: int — число ячеек по вертикали
        num_cols: int — число ячеек по горизонтали
        margin: int — отступ от краёв изображения
        thickness: int — толщина линий
    Returns:
        np.ndarray — изображение с сеткой линий
    """
    height = 2 * margin + cell_size * num_rows
    width = 2 * margin + cell_size * num_cols
    image = np.ones((height, width, 3), dtype=np.uint8) * 255  # белый фон
    for row in range(num_rows + 1):
        y = margin + row * cell_size
        cv2.line(image, (margin, y), (width - margin, y), color=(0, 0, 0), thickness=thickness)
    for col in range(num_cols + 1):
        x = margin + col * cell_size
        cv2.line(image, (x, margin), (x, height - margin), color=(0, 0, 0), thickness=thickness)
    return image
img = create_line_grid()
cv2.imwrite("grid_lines.png", img)
# ==== example\report\distortion.py ====
import cv2
import numpy as np
def distort_image_radially(image, k1, k2=0.0, k3=0.0):
    h, w = image.shape[:2]
    x_c, y_c = w / 2, h / 2
    xv, yv = np.meshgrid(np.arange(w), np.arange(h))
    dx = xv - x_c
    dy = yv - y_c
    r2 = dx ** 2 + dy ** 2
    L = 1 + k1 * r2 + k2 * r2 ** 2 + k3 * r2 ** 3
    map_x = (dx * L + x_c).astype(np.float32)
    map_y = (dy * L + y_c).astype(np.float32)
    distorted = cv2.remap(image, map_x, map_y,
                          interpolation=cv2.INTER_LINEAR,
                          borderMode=cv2.BORDER_CONSTANT,
                          borderValue=(255, 255, 255)
                          )
    return distorted
img = cv2.imread("grid_lines.png")  # путь к изображению
distorted = distort_image_radially(img, k1=-2e-6)
cv2.imwrite("distorted_pinkush.png", distorted)
# ==== example\synthetic\main.py ====
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d.art3d import Poly3DCollection
from mpl_toolkits.mplot3d import Axes3D
road_width = 8  # Ширина каждой полосы
intersection_size = 20  # Размер перекрестка
building_width = 15
building_height_left = 25
building_height_right = 30
traffic_light_height = 0
pole_height = 8
shift_x = 30
shift_y = road_width + 20
def shift_point(p):
    return [p[0] + shift_x, p[1] + shift_y, p[2]]
def shift_geometry(geom):
    return [shift_point(p) for p in geom]
def shift_object(obj):
    return {
        "type": obj["type"],
        "geometry": shift_geometry(obj["geometry"])
    }
def create_intersection():
    """Создает основной перекресток"""
    roads = []
    main_road = {
        "type": "road_main",
        "geometry": [
            [-40, -road_width, 0], [70, -road_width, 0],
            [70, road_width, 0], [-40, road_width, 0]
        ]
    }
    cross_road = {
        "type": "road_cross",
        "geometry": [
            [-road_width, -40, 0], [road_width, -40, 0],
            [road_width, 60, 0], [-road_width, 60, 0]
        ]
    }
    return [shift_object(main_road), shift_object(cross_road)]
def create_lane_markings():
    """Создает разметку полос движения - 3 белые линии на каждую дорогу"""
    markings = []
    full_lane_x = []
    full_lane_y = []
    y_positions = [-road_width / 2, 0, road_width / 2]  # 3 линии разметки
    for y_pos in y_positions:
        for x in np.arange(-40, 70, 6):
            if abs(x) > road_width + 2:  # Не рисуем разметку в зоне перекрестка
                markings.append({
                    "type": "lane_marking",
                    "geometry": [[x, y_pos, 0.0], [x + 3, y_pos, 0.0]]
                })
    for y_pos in y_positions:
        full_lane_x.append([shift_point([-20, y_pos, 0]), shift_point([20, y_pos, 0])])
    x_positions = [-road_width / 2, 0, road_width / 2]  # 3 линии разметки
    for x_pos in x_positions:
        for y in np.arange(-40, 60, 6):
            if abs(y) > road_width + 2:  # Не рисуем разметку в зоне перекрестка
                markings.append({
                    "type": "lane_marking",
                    "geometry": [[x_pos, y, 0.0], [x_pos, y + 3, 0.0]]
                })
    for x_pos in x_positions:
        full_lane_y.append([shift_point([x_pos, -20, 0]), shift_point([x_pos, 20, 0])])
    return [shift_object(obj) for obj in markings], full_lane_x, full_lane_y
def create_crosswalks():
    """Создает пешеходные переходы в границах проезжей части"""
    crosswalks = []
    stripe_width = 1.0
    crosswalk_width = 4.0  # ширина перехода (по направлению движения)
    road_span = 2 * road_width
    num_stripes = int(road_span // 1.5)
    start_x = -road_width + (road_span - num_stripes * 1.5) / 2  # центрируем
    start_y = -road_width + (road_span - num_stripes * 1.5) / 2
    for y_side in [-road_width - crosswalk_width / 2, road_width + crosswalk_width / 2]:
        for i in range(num_stripes):
            x_start = start_x + i * 1.5
            crosswalks.append({
                "type": "crosswalk",
                "geometry": [
                    [x_start, y_side - crosswalk_width / 2, 0],
                    [x_start + stripe_width, y_side - crosswalk_width / 2, 0],
                    [x_start + stripe_width, y_side + crosswalk_width / 2, 0],
                    [x_start, y_side + crosswalk_width / 2, 0]
                ]
            })
    for x_side in [-road_width - crosswalk_width / 2, road_width + crosswalk_width / 2]:
        for i in range(num_stripes):
            y_start = start_y + i * 1.5
            crosswalks.append({
                "type": "crosswalk",
                "geometry": [
                    [x_side - crosswalk_width / 2, y_start, 0.],
                    [x_side + crosswalk_width / 2, y_start, 0.0],
                    [x_side + crosswalk_width / 2, y_start + stripe_width, 0.],
                    [x_side - crosswalk_width / 2, y_start + stripe_width, 0.]
                ]
            })
    return [shift_object(obj) for obj in crosswalks]
def create_traffic_infrastructure():
    """Создает больше фонарных столбов и светофоров вокруг перекрестка"""
    poles = []
    traffic_lights = []
    main_pole_positions = [
        [-road_width - 4, -road_width - 4],
        [road_width + 4, -road_width - 4],
        [road_width + 4, road_width + 4],
        [-road_width - 4, road_width + 4]
    ]
    additional_pole_positions = [
        [-25, -road_width - 4],
        [25, -road_width - 4],
        [-25, road_width + 4],
        [-road_width - 4, -20],
        [-road_width - 4, 20],
        [road_width + 4, -20],
        [road_width + 4, 20]
    ]
    all_positions = main_pole_positions + additional_pole_positions
    for i, (x, y) in enumerate(all_positions):
        poles.append({
            "type": "pole",
            "geometry": [[x, y, 0], [x, y, pole_height]]
        })
        if i < 4:  # Первые 4 позиции - основные
            traffic_lights.append({
                "type": "traffic_light",
                "geometry": [
                    [x - 0.3, y - 0.3, pole_height],
                    [x + 0.3, y - 0.3, pole_height],
                    [x + 0.3, y + 0.3, pole_height],
                    [x - 0.3, y + 0.3, pole_height],
                    [x - 0.3, y - 0.3, pole_height + traffic_light_height],
                    [x + 0.3, y - 0.3, pole_height + traffic_light_height],
                    [x + 0.3, y + 0.3, pole_height + traffic_light_height],
                    [x - 0.3, y + 0.3, pole_height + traffic_light_height]
                ]
            })
    poles_shifted = [shift_object(p) for p in poles]
    lights_shifted = [shift_object(l) for l in traffic_lights]
    return poles_shifted, lights_shifted
def create_buildings():
    """Создает два здания по краям сцены"""
    buildings = []
    buildings.append({
        "type": "building",
        "geometry": [
            [20, road_width + 3, 0], [50, road_width + 3, 0], [50, 25, 0], [20, 25, 0],
            [20, road_width + 3, building_height_right], [50, road_width + 3, building_height_right],
            [50, 25, building_height_right], [20, 25, building_height_right]
        ]
    })
    return [shift_object(obj) for obj in buildings]
def draw_coordinate_systems(ax):
    """Рисует мировую систему координат на краю здания и систему координат камеры"""
    world_origin = [0, 0, 0]  # Край левого здания
    axis_length = 8
    ax.plot([world_origin[0], world_origin[0] + axis_length],
            [world_origin[1], world_origin[1]],
            [world_origin[2], world_origin[2]], 'r-', linewidth=4, label='World X')
    ax.plot([world_origin[0], world_origin[0]],
            [world_origin[1], world_origin[1] + axis_length],
            [world_origin[2], world_origin[2]], 'g-', linewidth=4, label='World Y')
    ax.plot([world_origin[0], world_origin[0]],
            [world_origin[1], world_origin[1]],
            [world_origin[2], world_origin[2] + axis_length], 'b-', linewidth=4, label='World Z')
    from scipy.spatial.transform import Rotation
    R = Rotation.from_euler('zxy', [-142.28, 49.46, 172.08], degrees=True).as_matrix()
    camera_origin = [world_origin[0], world_origin[1], world_origin[2] + 25]
    camera_axis_length = 4
    x_axis = np.array([camera_axis_length, 0, 0])
    y_axis = np.array([0, camera_axis_length, 0])
    z_axis = np.array([0, 0, camera_axis_length])
    x_axis_rot = R.T @ x_axis
    y_axis_rot = R.T @ y_axis
    z_axis_rot = R.T @ z_axis
    ax.plot([camera_origin[0], camera_origin[0] + x_axis_rot[0]],
            [camera_origin[1], camera_origin[1] + x_axis_rot[1]],
            [camera_origin[2], camera_origin[2] + x_axis_rot[2]],
            color='darkred', linewidth=3, linestyle='--', label='Camera X')
    ax.plot([camera_origin[0], camera_origin[0] + y_axis_rot[0]],
            [camera_origin[1], camera_origin[1] + y_axis_rot[1]],
            [camera_origin[2], camera_origin[2] + y_axis_rot[2]],
            color='darkgreen', linewidth=3, linestyle='--', label='Camera Y')
    ax.plot([camera_origin[0], camera_origin[0] + z_axis_rot[0]],
            [camera_origin[1], camera_origin[1] + z_axis_rot[1]],
            [camera_origin[2], camera_origin[2] + z_axis_rot[2]],
            color='darkblue', linewidth=3, linestyle='--', label='Camera Z')
    ax.text(world_origin[0] + axis_length + 1, world_origin[1], world_origin[2],
            'X_world', fontsize=12, color='red', weight='bold')
    ax.text(world_origin[0], world_origin[1] + axis_length + 1, world_origin[2],
            'Y_world', fontsize=12, color='green', weight='bold')
    ax.text(world_origin[0], world_origin[1], world_origin[2] + axis_length + 1,
            'Z_world', fontsize=12, color='blue', weight='bold')
    ax.text(camera_origin[0] + x_axis_rot[0] + 0.5, camera_origin[1] + x_axis_rot[1],
            camera_origin[2] + x_axis_rot[2], 'X_cam', fontsize=10, color='darkred', weight='bold')
    ax.text(camera_origin[0] + y_axis_rot[0], camera_origin[1] + y_axis_rot[1] + 0.5,
            camera_origin[2] + y_axis_rot[2], 'Y_cam', fontsize=10, color='darkgreen', weight='bold')
    ax.text(camera_origin[0] + z_axis_rot[0], camera_origin[1] + z_axis_rot[1],
            camera_origin[2] + z_axis_rot[2] + 0.5, 'Z_cam', fontsize=10, color='darkblue', weight='bold')
roads = create_intersection()
lane_markings, full_lane_x, full_lane_y = create_lane_markings()
crosswalks = create_crosswalks()
poles, traffic_lights = create_traffic_infrastructure()
buildings = create_buildings()
scene_objects = roads + lane_markings + crosswalks + poles + traffic_lights + buildings
fig = plt.figure(figsize=(16, 12))
ax = fig.add_subplot(111, projection='3d')
ax.set_title("Сцена дорожного движения с перекрестком\n(с мировой системой координат и системой координат камеры)",
             fontsize=14, pad=20)
ax.set_xlabel("X (м)", fontsize=12)
ax.set_ylabel("Y (м)", fontsize=12)
ax.set_zlabel("Z (м)", fontsize=12)
for obj in scene_objects:
    obj_type = obj["type"]
    geometry = obj["geometry"]
    if obj_type in ["road_main", "road_cross"]:
        verts = [geometry]
        ax.add_collection3d(Poly3DCollection(verts, facecolor='darkgray', alpha=0.8, edgecolor='black'))
    elif obj_type == "building":
        verts = [
            [geometry[i] for i in [0, 1, 2, 3]],  # Нижняя грань
            [geometry[i] for i in [4, 5, 6, 7]],  # Верхняя грань
            [geometry[i] for i in [0, 1, 5, 4]],  # Боковые грани
            [geometry[i] for i in [1, 2, 6, 5]],
            [geometry[i] for i in [2, 3, 7, 6]],
            [geometry[i] for i in [3, 0, 4, 7]]
        ]
        ax.add_collection3d(Poly3DCollection(verts, facecolor='lightblue', alpha=0.7, edgecolor='navy'))
    elif obj_type == "traffic_light":
        verts = [
            [geometry[i] for i in [0, 1, 2, 3]],  # Нижняя грань
            [geometry[i] for i in [4, 5, 6, 7]],  # Верхняя грань
            [geometry[i] for i in [0, 1, 5, 4]],  # Боковые грани
            [geometry[i] for i in [1, 2, 6, 5]],
            [geometry[i] for i in [2, 3, 7, 6]],
            [geometry[i] for i in [3, 0, 4, 7]]
        ]
        ax.add_collection3d(Poly3DCollection(verts, facecolor='red', alpha=0.9, edgecolor='darkred'))
    elif obj_type == "crosswalk":
        verts = [geometry]
        ax.add_collection3d(Poly3DCollection(verts, facecolor='white', alpha=0.9, edgecolor='black'))
    else:
        if len(geometry) >= 2:
            xs, ys, zs = zip(*geometry)
            color_map = {
                "lane_marking": 'white',
                "pole": 'black'
            }
            color = color_map.get(obj_type, 'blue')
            linewidth = 4 if obj_type == "pole" else 3
            ax.plot(xs, ys, zs, color=color, linewidth=linewidth)
draw_coordinate_systems(ax)
ax.view_init(elev=25, azim=45)
ax.legend(loc='upper left', bbox_to_anchor=(0, 1))
plt.tight_layout()
def project_building_faces(building_geometry, camera):
    """
    Проецирует здание как набор отдельных граней
    building_geometry: 8 точек параллелепипеда [x,y,z]
    camera: объект камеры с методом project_direct
    """
    faces = [
        [0, 1, 2, 3],  # Нижняя грань (основание)
        [4, 5, 6, 7],  # Верхняя грань (крыша)
        [0, 1, 5, 4],  # Передняя грань
        [1, 2, 6, 5],  # Правая грань
        [2, 3, 7, 6],  # Задняя грань
        [3, 0, 4, 7]  # Левая грань
    ]
    projected_faces = []
    for face_indices in faces:
        face_points = [building_geometry[i] for i in face_indices]
        projected_points = []
        for pt in face_points:
            pt_proj = camera.project_direct(PointND(pt, add_weight=True)).get()
            projected_points.append(pt_proj)
        projected_faces.append(np.array(projected_points))
    return projected_faces
def draw_building_with_faces(ax, building_obj, camera, color='lightblue'):
    """
    Отрисовывает здание с правильным разделением на грани
    """
    geometry = building_obj['geometry']
    projected_faces = project_building_faces(geometry, camera)
    for i, face in enumerate(projected_faces):
        if len(face) >= 3:  # Проверяем что грань имеет достаточно точек
            alpha = 0.6 if i < 2 else 0.8  # Верх/низ более прозрачные
            ax.fill(face[:, 0], face[:, 1], color=color, alpha=alpha, edgecolor='navy', linewidth=1)
print("Сцена создана успешно!")
print("Компоненты сцены:")
print("- Перекресток с двумя пересекающимися дорогами")
print("- Белая разметка полос движения (по 3 линии на каждую дорогу)")
print("- Широкие пешеходные переходы (4м) на всех четырех сторонах")
print("- 12 фонарных столбов (4 со светофорами на углах + 8 дополнительных)")
print("- Два здания по краям сцены")
print("- Мировая система координат (на краю левого здания)")
print("- Система координат камеры (над мировой системой, углы Тейта-Брайана)")
from source.core import Camera, PointND
camera = Camera(size=(1080, 1920))
camera.set_params_from_list([1200, -150, 55, 175, 0, 0, 25])
print(f'Исходные координаты{[1200, -150, 55, 175, 0, 0, 25]}')
fig, ax = plt.subplots(figsize=(12, 9))
color_map = {
    'road_main': 'darkgray',
    'road_cross': 'darkgray',
    'building': 'lightblue',
    'crosswalk': 'white',
    'traffic_light': 'red',
    'pole': 'black',
    'lane_marking': 'white'
}
for obj in scene_objects:
    color = color_map.get(obj['type'], 'green')
    geometry = obj['geometry']
    if obj['type'] == 'building':
        draw_building_with_faces(ax, obj, camera, color)
    else:
        projected = []
        for pt in geometry:
            pt_proj = camera.project_direct(PointND(pt, add_weight=True)).get()
            projected.append(pt_proj)
        projected = np.array(projected)
        if obj['type'] in ['road_main', 'road_cross', 'crosswalk', 'traffic_light']:
            if len(projected) >= 3:
                ax.fill(projected[:, 0], projected[:, 1], color=color, alpha=0.8)
        else:
            if len(projected) == 2:
                ax.plot(projected[:, 0], projected[:, 1], color=color, linewidth=2)
import numpy as np
def get_direction(p1, p2):
    v = np.array(p2) - np.array(p1)
    v[2] = 0  # игнорируем Z
    norm = np.linalg.norm(v)
    return v / norm if norm > 0 else v
angle_thresh = np.deg2rad(15)
target_dir = np.array([1, 0])  # направление вдоль X
lane_markings = [obj for obj in scene_objects if obj['type'] == 'lane_marking']
horizontal_markings = []
for obj in lane_markings:
    p1, p2 = obj['geometry']
    direction = get_direction(p1, p2)[:2]
    angle = np.arccos(np.clip(np.dot(direction, target_dir), -1.0, 1.0))
    if angle < angle_thresh:
        horizontal_markings.append(obj)
projected_segments_horizont = []
for obj in horizontal_markings:
    p1_3d, p2_3d = obj['geometry']
    p1_2d = camera.project_direct(PointND(p1_3d, add_weight=True)).get()
    p2_2d = camera.project_direct(PointND(p2_3d, add_weight=True)).get()
    if np.random.rand() < 0.5:  # 30% случаев — добавляем шум
        p1_2d += np.random.randint(-2, 2, size=2)
        p2_2d += np.random.randint(-2, 2, size=2)
    projected_segments_horizont.append([p1_2d, p2_2d])
projected_segments_pole = []
projected_segments_pole_ideal = []
pole = [obj for obj in scene_objects if obj['type'] == 'pole']
for obj in pole:
    p1_3d, p2_3d = obj['geometry']
    p1_2d = camera.project_direct(PointND(p1_3d, add_weight=True)).get()  # + np.random.randint(-30, 30, size=2)
    p2_2d = camera.project_direct(PointND(p2_3d, add_weight=True)).get()  # + np.random.randint(-30, 30, size=2)
    projected_segments_pole_ideal.append([p1_2d, p2_2d])
    if np.random.rand() < 0.5:  # 30% случаев — добавляем шум
        p1_2d += np.random.randint(-10, 10, size=2)
        p2_2d += np.random.randint(-10, 10, size=2)
    projected_segments_pole.append([p1_2d, p2_2d])
from source.vp_detection import VanishingPointEstimatorManual
from source.calibration import VanishingPointCalibration, RefineOptimizer, CalibrationPipeline
from source.calibration.refine import residual_interline_distance, residual_reprojection_line, \
    residual_reprojection_point, residual_line_length, residual_orthogonality_error, residual_parallel_group
vp1_manual = VanishingPointEstimatorManual().estimate(projected_segments_horizont[::5])
vp3_manual = VanishingPointEstimatorManual().estimate(projected_segments_pole[::5])
print(f'Точки схода: {vp1_manual, vp3_manual}')
camera_new = Camera(size=(1080, 1920))
vp_init = VanishingPointCalibration(camera_new)
vp_init.set_vanishing_points(vpX=vp1_manual, vpZ=vp3_manual)
def draw():
    global scale, p1, p2
    scale = 1
    ax.set_xlim(0, camera.size[1] * scale)
    ax.set_ylim(camera.size[0] * scale, 0)  # Важно: инвертируем ось Y, как в изображении
    ax.set_title("Проекция сцены на изображение")
    plt.show()
pole = [obj for obj in scene_objects if obj['type'] == 'pole']
data_direct_optimize_point_to_point = []
for obj in pole[:4]:
    p1_3d, _ = obj['geometry']
    p1_2d = camera.project_direct(PointND(p1_3d, add_weight=True)).get()
    data = {"pixel": p1_2d + np.random.randint(-10, 10, size=2),
            "gps": p1_3d}  # + np.random.randint(-1, 1, size=3)
    ax.scatter(*data['pixel'], c='r', label='Ошибка проекции')
    data_direct_optimize_point_to_point.append(
        data
    )
    data_direct_optimize = {
        "point_to_point": data_direct_optimize_point_to_point[:-5],
    }
    residualds_blocs = [
        lambda cam, data: (
            np.array(residual_reprojection_point(cam, data, group='point_to_point')),
            'point_to_point'
        ),
    ]
refine = RefineOptimizer(
    camera=camera_new,
    residual_blocks=residualds_blocs,
    mask=[0, 1, 2, 3, 6],
    bounds=[[500, -360, -360, -360, 10],
            [1500, 360, 360, 360, 27]],
    method='trf'
)
pipeline = CalibrationPipeline(
    init_stage=vp_init,
    refine_stages=[refine],
    n_iter=1,
)
pole = [obj for obj in scene_objects if obj['type'] == 'crosswalk']
crosswalk_dataset = []
for obj in pole[:6]:
    p1, p2, p3, p4 = obj['geometry']
    if np.linalg.norm(np.array(p1) - np.array(p4)) == 4:
        p1 = camera.project_direct(PointND(p1, add_weight=True)).get() + np.random.randint(-3, 3, size=2)
        p4 = camera.project_direct(PointND(p4, add_weight=True)).get() + np.random.randint(-3, 3, size=2)
        crosswalk_dataset.append([p1, p4])
        ax.plot([p1[0], p4[0]], [p1[1], p4[1]], color='red', linewidth=2, label='Ошибка длины отрезков')
    elif np.linalg.norm(np.array(p1) - np.array(p2)) == 4:
        p1 = camera.project_direct(PointND(p1, add_weight=True)).get() + np.random.randint(-3, 3, size=2)
        p2 = camera.project_direct(PointND(p2, add_weight=True)).get() + np.random.randint(-3, 3, size=2)
        crosswalk_dataset.append([p1, p2])
data_direct_optimize = {
    "point_to_point": data_direct_optimize_point_to_point[::2],
    "line_length": crosswalk_dataset[::10],
}
residualds_blocs = [
    lambda cam, data: (
        np.array(residual_reprojection_point(cam, data, group='point_to_point')) * (1 / 100),
        'point_to_point'
    ),
    lambda cam, data: (np.array(residual_line_length(cam, data, group='line_length', expected=4)) * (1),
                       'line_length')
]
refine = RefineOptimizer(
    camera=camera_new,
    residual_blocks=residualds_blocs,
    mask=[0, 1, 2, 3, 6],
    bounds=[[500, -360, -360, -360, 10],
            [1500, 360, 360, 360, 27]],
    method='trf'
)
pipeline = CalibrationPipeline(
    init_stage=vp_init,
    refine_stages=[refine],
    n_iter=1,
)
from source.calibration.base import RESUALDS
full_lane_x_optimize = []
for p1, p2 in full_lane_x:
    p1 = camera.project_direct(PointND(p1, add_weight=True)).get() + np.random.randint(-10, 10, size=2)
    p2 = camera.project_direct(PointND(p2, add_weight=True)).get() + np.random.randint(-10, 10, size=2)
    full_lane_x_optimize.append([p1, p2])
    ax.plot([p1[0], p2[0]], [p1[1], p2[1]], color='lime', linewidth=2, label='Ошибка межлинейного расстояния')
full_lane_y_optimize = []
for p1, p2 in full_lane_y:
    p1 = camera.project_direct(PointND(p1, add_weight=True)).get()  # + np.random.randint(-20, 20, size=2)
p2 = camera.project_direct(PointND(p2, add_weight=True)).get()  # + np.random.randint(-20, 20, size=2)
full_lane_y_optimize.append([p1, p2])
ax.set_xlim(0, camera.size[1])
ax.set_ylim(camera.size[0], 0)
data_direct_optimize = {
    "point_to_point": data_direct_optimize_point_to_point[::3],
    "line_length": crosswalk_dataset[::10],
    "dist_betweeen_line_1": full_lane_x_optimize,
    "dist_betweeen_line_2": full_lane_y_optimize,
}
residualds_blocs = [
    lambda cam, data: (
        np.array(residual_reprojection_point(cam, data, group='point_to_point')) * (1 / 100),
        'Ошибка проекции'
    ),
    lambda cam, data: (np.array(residual_line_length(cam, data, group='line_length', expected=4)) * (1),
                       'Ошибка длины отрезков'),
]
refine = RefineOptimizer(
    camera=camera_new,
    residual_blocks=residualds_blocs,
    mask=[0, 1, 2, 3, 6],
    bounds=[[500, -360, -360, -360, 10],
            [1500, 360, 360, 360, 27]],
    method='trf'
)
pipeline = CalibrationPipeline(
    init_stage=vp_init,
    refine_stages=[refine],
    n_iter=1,
)
pipeline.run(camera=camera_new, data=data_direct_optimize)
print("delta", - np.array(camera_new.get_params()) + np.array(camera.get_params()))
from source.calibration.base import RESUALDS
from calibration.debug import plot_residuals_comparison
handles, labels = plt.gca().get_legend_handles_labels()
unique = dict(zip(labels, handles))
plt.legend(unique.values(), unique.keys())
plot_residuals_comparison(RESUALDS)
# ==== source\__init__.py ====
"""
road_camera_calibration_toolkit
~~~~~~~~~~~~~~~~~~~~
Библиотека для калибровки камеры видеонаблюдения за дорожным движением на основе точек схода, структурных ограничений сцены
и данных с GPS.
"""
from .calibration import CalibrationPipeline
from .calibration import RefineOptimizer
from .core import Camera, PointND
from .calibration import VanishingPointCalibration
__all__ = [
    "CalibrationPipeline",
    "RefineOptimizer",
    "Camera",
    "VanishingPointCalibration",
    "PointND",
]
__version__ = "0.2.0"
__author__ = "Акмурзин Михаил"
__email__ = "akmurzinmihail@gmail.com"# ==== source\annotation_tools\__init__.py ====
from .data_markup_tool import AnnotationTool
from .annotation_parser import AnnotationParser
from .data_preparation import load_lines, load_lines_from_json
__all__ = [
    "AnnotationTool",
    "AnnotationParser",
    "load_lines",
    "load_lines_from_json"
]
# ==== source\annotation_tools\annotation_parser.py ====
import json
from collections import defaultdict
class AnnotationParser:
    def __init__(self, filepath):
        self.filepath = filepath
        self.annotations = {"line": {}, "point": {}}
        self._load()
    def _load(self):
        with open(self.filepath, "r", encoding="utf-8") as f:
            data = json.load(f)
        self.annotations["point"] = data.get("point", {})
        self.annotations["line"] = data.get("line", {})
    def get_all_classes(self):
        return sorted(set(self.annotations["point"]) | set(self.annotations["line"]))
    def get_points_by_class(self, class_name):
        return [ann["image"] for ann in self.annotations["point"].get(class_name, []) if "image" in ann]
    def get_lines_by_class(self, class_name):
        return [ann["image"] for ann in self.annotations["line"].get(class_name, []) if "image" in ann]
    def get_all_points(self):
        points = []
        for ann_list in self.annotations["point"].values():
            points.extend([ann["image"] for ann in ann_list if "image" in ann])
        return points
    def get_all_lines(self):
        lines = []
        for ann_list in self.annotations["line"].values():
            lines.extend([ann["image"] for ann in ann_list if "image" in ann])
        return lines
    def count_per_class(self):
        stats = {}
        for cls in self.get_all_classes():
            stats[cls] = {
                "point": len(self.annotations["point"].get(cls, [])),
                "line": len(self.annotations["line"].get(cls, [])),
            }
        return stats
    def get_gps_points_by_class(self, class_name):
        return [ann["gps"] for ann in self.annotations["point"].get(class_name, []) if "gps" in ann]
    def get_gps_lines_by_class(self, class_name):
        return [ann["gps"] for ann in self.annotations["line"].get(class_name, []) if "gps" in ann]
    def get_lines_with_gps_and_pixel(self, class_name):
        """
        Возвращает список словарей с пиксельными и GPS-координатами для линий указанного класса.
        Формат: [{"pixel": [[x1, y1], [x2, y2]], "gps": [[lat1, lon1, alt1], [lat2, lon2, alt2]]}, ...]
        """
        result = []
        for ann in self.annotations["line"].get(class_name, []):
            if "image" in ann and "gps" in ann:
                result.append({"pixel": ann["image"], "gps": ann["gps"]})
        return result
    def get_points_with_gps_and_pixel(self, class_name):
        """
        Возвращает список словарей с пиксельными и GPS-координатами для линий указанного класса.
        Формат: [{"pixel": [[x1, y1], [x2, y2]], "gps": [[lat1, lon1, alt1], [lat2, lon2, alt2]]}, ...]
        """
        result = []
        for ann in self.annotations["point"].get(class_name, []):
            if "image" in ann and "gps" in ann:
                result.append({"pixel": ann["image"], "gps": ann["gps"]})
        return result
if __name__ == "__main__":
    parser = AnnotationParser("../../example/karls_marks/data/data_full.json")
    print("Классы:", parser.get_all_classes())
    print("Точек всего:", len(parser.get_all_points()))
    print("Линий всего:", len(parser.get_all_lines()))
    for cls, s in parser.count_per_class().items():
        print(f"{cls}: {s['point']} точек, {s['line']} линий")
# ==== source\annotation_tools\data_markup_tool.py ====
import sys
import json
import hashlib
import colorsys
from PyQt5.QtWidgets import (
    QApplication, QMainWindow, QLabel, QPushButton, QFileDialog,
    QVBoxLayout, QHBoxLayout, QComboBox, QWidget
)
from PyQt5.QtGui import QPixmap, QPainter, QPen, QColor, QFont
from PyQt5.QtCore import Qt, QPoint
from PyQt5.QtGui import QMouseEvent
from PyQt5.QtGui import QColor
PALETTE = [
    "#e6194b",  # насыщенный красный
    "#3cb44b",  # яркий зелёный
    "#ffe119",  # яркий жёлтый
    "#4363d8",  # глубокий синий
    "#f58231",  # оранжевый
    "#911eb4",  # фиолетовый
    "#00ced1",  # насыщенный бирюзовый
    "#f032e6",  # фуксия
    "#aaff00",  # ядовито-салатовый
    "#ff1493",  # тёмно-розовый
    "#008080",  # тёмная бирюза
    "#6a5acd",  # индиго
    "#8b4513",  # насыщенный коричневый
    "#ff4500",  # красно-оранжевый
    "#800000",  # бордовый
    "#00ff7f",  # неоново-зелёный
    "#808000",  # оливковый
    "#ff8c00",  # тёмно-оранжевый
    "#00008b",  # тёмно-синий
    "#000000",  # чёрный
]
class AnnotationTool(QMainWindow):
    def __init__(self):
        super().__init__()
        self.setWindowTitle("Annotation Tool")
        self.image_label = QLabel()
        self.image_label.setMouseTracking(True)
        self.image_label.mousePressEvent = self.mouse_press_event
        self.image_label.mouseMoveEvent = self.mouse_move_event
        self.image_label.mouseReleaseEvent = self.mouse_release_event
        self.load_btn = QPushButton("Загрузить изображение")
        self.load_btn.clicked.connect(self.load_image)
        self.save_btn = QPushButton("Сохранить аннотации")
        self.save_btn.clicked.connect(self.save_annotations)
        self.load_ann_btn = QPushButton("Загрузить аннотации")
        self.load_ann_btn.clicked.connect(self.load_annotations)
        self.mode_selector = QComboBox()
        self.mode_selector.addItem("Точка", "point")
        self.mode_selector.addItem("Линия", "line")
        self.mode_selector.addItem("Кривая", "curve")
        self.mode_selector.currentIndexChanged.connect(self.toggle_gps_fields)
        self.class_selector = QComboBox()
        self.class_selector.setEditable(True)
        self.class_selector.addItem("all")
        self.class_selector.addItem("default")
        from PyQt5.QtWidgets import QLineEdit
        self.gps_input_1 = QLineEdit()
        self.gps_input_1.setPlaceholderText("GPS 1: широта, долгота")
        self.gps_input_2 = QLineEdit()
        self.gps_input_2.setPlaceholderText("GPS 2: широта, долгота")
        self.gps_input_2.hide()  # По умолчанию скрыто
        self.add_class_btn = QPushButton("Добавить класс")
        self.add_class_btn.clicked.connect(self.add_class)
        top_bar = QHBoxLayout()
        top_bar.addWidget(self.load_btn)
        top_bar.addWidget(self.load_ann_btn)
        top_bar.addWidget(self.save_btn)
        mode_label = QLabel("Режим:")
        mode_label.setFixedWidth(50)
        mode_layout = QHBoxLayout()
        mode_layout.addWidget(mode_label)
        mode_layout.addWidget(self.mode_selector)
        mode_layout.setStretch(0, 0)
        mode_layout.setStretch(1, 1)
        top_bar.addLayout(mode_layout)
        class_label = QLabel("Класс:")
        class_label.setFixedWidth(50)
        class_layout = QHBoxLayout()
        class_layout.addWidget(class_label)
        class_layout.addWidget(self.class_selector)
        class_layout.setStretch(0, 0)
        class_layout.setStretch(1, 1)
        top_bar.addLayout(class_layout)
        layout = QVBoxLayout()
        layout.addLayout(top_bar)
        gps_layout = QHBoxLayout()
        self.gps_status = QLabel("Ничего не выбрано")
        layout.addWidget(self.gps_status)  # добавляется ПЕРЕД gps_layout
        gps_layout.addWidget(self.gps_input_1)
        gps_layout.addWidget(self.gps_input_2)
        self.update_gps_btn = QPushButton("Обновить GPS")
        self.update_gps_btn.clicked.connect(self.update_selected_gps)
        gps_layout.addWidget(self.update_gps_btn)
        self.clear_gps_btn = QPushButton("Сбросить GPS")
        self.clear_gps_btn.clicked.connect(self.clear_gps)
        gps_layout.addWidget(self.clear_gps_btn)
        layout.addLayout(gps_layout)
        layout.addWidget(self.image_label)
        container = QWidget()
        container.setLayout(layout)
        self.setCentralWidget(container)
        self.image = None
        self.scaled_image = None
        self.display_scale = 1.0
        self.annotations = {"point": {}, "line": {}, "curve": {}}
        self.current_line = []
        self.selected = None
        self.dragging = False
        self.hover = None
        self.current_curve = []
        self.highlighted_line = None
        gps_layout.addWidget(self.update_gps_btn)
    def auto_update_gps(self):
        if self.selected:
            self.update_selected_gps()
    def clear_gps(self):
        if not self.selected:
            self.gps_input_1.clear()
            self.gps_input_2.clear()
            return
        self.gps_input_1.clear()
        self.gps_input_2.clear()
        self.update_selected_gps()
    def update_selected_gps(self):
        if not self.selected:
            return
        kind, cls, item_idx, pt_idx = self.selected
        ann = self.annotations[kind][cls][item_idx]
        gps1_text = self.gps_input_1.text().strip()
        gps2_text = self.gps_input_2.text().strip()
        gps1 = self.parse_gps(gps1_text) if gps1_text else None
        gps2 = self.parse_gps(gps2_text) if gps2_text else None
        if kind == "point":
            ann["gps"] = gps1
        elif kind == "line":
            if "gps" not in ann:
                ann["gps"] = [None, None]
            elif not isinstance(ann["gps"], list):
                ann["gps"] = [None, None]
            elif len(ann["gps"]) < 2:
                ann["gps"] = ann["gps"] + [None] * (2 - len(ann["gps"]))
            ann["gps"][0] = gps1
            ann["gps"][1] = gps2
        elif kind == "curve":
            if gps1 is not None:
                ann["gps"] = [gps1] * len(ann["image"])
            else:
                ann["gps"] = [None] * len(ann["image"])
        print(f"GPS после обновления для {kind} класса '{cls}' №{item_idx}:", ann.get("gps"))
        self.update_display()
    def toggle_gps_fields(self):
        mode = self.mode_selector.currentData()
        if mode == "line":
            self.gps_input_2.show()
        else:
            self.gps_input_2.hide()
    def parse_gps(self, text):
        try:
            lat_str, lon_str = text.split(",")
            return float(lat_str.strip()), float(lon_str.strip())
        except:
            return None
    def load_image(self):
        path, _ = QFileDialog.getOpenFileName(self, "Выберите изображение")
        if not path:
            return
        self.image = QPixmap(path)
        self.update_scale()
        self.update_display()
    def update_scale(self):
        max_width = 1280
        if self.image.width() > max_width:
            self.display_scale = max_width / self.image.width()
            self.scaled_image = self.image.scaledToWidth(max_width, Qt.SmoothTransformation)
        else:
            self.display_scale = 1.0
            self.scaled_image = self.image
    def get_mouse_pos(self, event):
        x = event.pos().x() / self.display_scale
        y = event.pos().y() / self.display_scale
        return int(x), int(y)
    def mouse_press_event(self, event):
        if not self.image:
            return
        x, y = self.get_mouse_pos(event)
        cls = self.class_selector.currentText().strip()
        mode = self.mode_selector.currentData()
        gps1_text = self.gps_input_1.text().strip()
        gps2_text = self.gps_input_2.text().strip()
        gps1 = self.parse_gps(gps1_text) if gps1_text else None
        gps2 = self.parse_gps(gps2_text) if gps2_text else None
        if cls and self.class_selector.findText(cls) == -1:
            self.class_selector.addItem(cls)
        if event.button() == Qt.RightButton:
            self.try_delete_nearest(x, y)
            return
        if event.button() == Qt.LeftButton:
            target = self.find_nearest_point(x, y)
            if target:
                kind, cls, item_idx, pt_idx = target
                ann = self.annotations[kind][cls][item_idx]
                self.selected = target
                self.dragging = True
                self.gps_status.setText(f"Редактируется {kind.upper()} – класс '{cls}' №{item_idx}")
                self.gps_input_1.setText("")
                self.gps_input_2.setText("")
                if kind == "line":
                    self.highlighted_line = (cls, item_idx)
                else:
                    self.highlighted_line = None
                if kind == "point" and ann.get("gps"):
                    lat, lon = ann["gps"]
                    self.gps_input_1.setText(f"{lat:.6f}, {lon:.6f}")
                elif kind == "line" and ann.get("gps"):
                    gps_list = ann["gps"]
                    if gps_list and len(gps_list) >= 2:
                        if gps_list[0]:
                            lat1, lon1 = gps_list[0]
                            self.gps_input_1.setText(f"{lat1:.6f}, {lon1:.6f}")
                        if gps_list[1]:
                            lat2, lon2 = gps_list[1]
                            self.gps_input_2.setText(f"{lat2:.6f}, {lon2:.6f}")
                return
            else:
                self.selected = None
                self.highlighted_line = None
                self.gps_status.setText("Ничего не выбрано")
                self.update_display()
        if mode == "point":
            self.annotations["point"].setdefault(cls, []).append({
                "image": (x, y),
                "gps": gps1
            })
        elif mode == "line":
            self.current_line.append((x, y))
            if len(self.current_line) == 2:
                self.annotations["line"].setdefault(cls, []).append({
                    "image": self.current_line.copy(),
                    "gps": [gps1, gps2]
                })
                self.current_line.clear()
        elif mode == "curve":
            self.current_curve.append((x, y))
            if event.type() == QMouseEvent.MouseButtonDblClick and len(self.current_curve) >= 2:
                gps_list = [gps1] * len(self.current_curve) if gps1 else [None] * len(self.current_curve)
                self.annotations["curve"].setdefault(cls, []).append({
                    "image": self.current_curve.copy(),
                    "gps": gps_list
                })
                self.current_curve.clear()
        self.update_display()
    def mouse_move_event(self, event):
        x, y = self.get_mouse_pos(event)
        if self.dragging and self.selected:
            kind, cls, item, pt_idx = self.selected
            try:
                if kind == "point":
                    self.annotations["point"][cls][item]["image"] = (x, y)
                elif kind == "line":
                    self.annotations["line"][cls][item]["image"][pt_idx] = (x, y)
                elif kind == "curve":
                    self.annotations["curve"][cls][item]["image"][pt_idx] = (x, y)
            except (KeyError, IndexError):
                self.dragging = False
                self.selected = None
        else:
            self.hover = self.find_nearest_point(x, y)
        self.update_display()
    def mouse_release_event(self, event):
        self.dragging = False
        self.highlighted_line = None
    def try_delete_nearest(self, x, y, threshold=10):
        target = self.find_nearest_point(x, y, threshold)
        if not target:
            return
        kind, cls, item_idx, pt_idx = target
        if kind == "point":
            del self.annotations["point"][cls][item_idx]
            if not self.annotations["point"][cls]:
                del self.annotations["point"][cls]
        elif kind == "line":
            del self.annotations["line"][cls][item_idx]
            if not self.annotations["line"][cls]:
                del self.annotations["line"][cls]
        elif kind == "curve":
            ann = self.annotations["curve"][cls][item_idx]
            if "image" in ann and 0 <= pt_idx < len(ann["image"]):
                del ann["image"][pt_idx]
                if "gps" in ann and len(ann["gps"]) > pt_idx:
                    del ann["gps"][pt_idx]
                if len(ann["image"]) < 2:
                    del self.annotations["curve"][cls][item_idx]
                if not self.annotations["curve"][cls]:
                    del self.annotations["curve"][cls]
        self.hover = None
        self.selected = None
        self.update_display()
    def find_nearest_point(self, x, y, threshold=10):
        for kind in ["point", "line", "curve"]:
            for cls, items in self.annotations[kind].items():
                for i, ann in enumerate(items):
                    item = ann["image"]
                    if kind == "point":
                        px, py = item
                        if abs(px - x) < threshold and abs(py - y) < threshold:
                            return (kind, cls, i, 0)
                    elif kind == "line":
                        for j, (px, py) in enumerate(item):
                            if abs(px - x) < threshold and abs(py - y) < threshold:
                                return (kind, cls, i, j)
                    elif kind == "curve":
                        for j, (px, py) in enumerate(item):
                            if abs(px - x) < threshold and abs(py - y) < threshold:
                                return (kind, cls, i, j)
        return None
    def get_color(self, name):
        h = int(hashlib.md5(name.encode()).hexdigest(), 16)
        color_hex = PALETTE[h % len(PALETTE)]
        return QColor(color_hex)
    def update_display(self):
        if not self.image:
            return
        selected_cls = self.class_selector.currentText().strip()
        pix = QPixmap(self.scaled_image)
        painter = QPainter(pix)
        painter.setFont(QFont("Arial", 10))
        for kind in ["point", "line", "curve"]:
            for cls, items in self.annotations[kind].items():
                if selected_cls != "all" and cls != selected_cls:
                    continue
                color = self.get_color(cls)
                for i, ann in enumerate(items):
                    item = ann["image"]
                    gps = ann.get("gps", None)
                    if kind == "point":
                        x, y = [int(p * self.display_scale) for p in item]
                        pen = QPen(QColor("yellow") if self.hover == (kind, cls, i, 0) else color, 4)
                        painter.setPen(pen)
                        painter.drawEllipse(QPoint(x, y), 5, 5)
                        painter.drawText(x + 8, y - 8, cls)
                        if gps and isinstance(gps, (list, tuple)) and len(gps) == 2:
                            if isinstance(gps[0], (int, float)) and isinstance(gps[1], (int, float)):
                                painter.setPen(QPen(color, 1))
                                painter.drawText(x + 8, y + 12, f"{gps[0]:.6f}, {gps[1]:.6f}")
                    elif kind == "line":
                        (x1, y1), (x2, y2) = item
                        sx1, sy1 = int(x1 * self.display_scale), int(y1 * self.display_scale)
                        sx2, sy2 = int(x2 * self.display_scale), int(y2 * self.display_scale)
                        is_selected = (self.highlighted_line == (cls, i))
                        line_pen = QPen(QColor("yellow") if is_selected else color, 6 if is_selected else 4)
                        painter.setPen(line_pen)
                        painter.drawLine(QPoint(sx1, sy1), QPoint(sx2, sy2))
                        for j, (px, py) in enumerate(item):
                            sx, sy = int(px * self.display_scale), int(py * self.display_scale)
                            if is_selected:
                                if j == 0:
                                    painter.setPen(QPen(QColor("limegreen"), 6))
                                    painter.setBrush(QColor("limegreen"))
                                    painter.drawEllipse(QPoint(sx, sy), 6, 6)
                                    painter.setPen(QPen(QColor("black"), 1))
                                    painter.drawText(sx + 8, sy - 8, "A")
                                elif j == 1:
                                    painter.setPen(QPen(QColor("red"), 6))
                                    painter.setBrush(QColor("red"))
                                    painter.drawEllipse(QPoint(sx, sy), 6, 6)
                                    painter.setPen(QPen(QColor("black"), 1))
                                    painter.drawText(sx + 8, sy - 8, "B")
                            else:
                                is_hover = self.hover == (kind, cls, i, j)
                                painter.setPen(QPen(QColor("yellow") if is_hover else color, 3))
                                painter.drawEllipse(QPoint(sx, sy), 4, 4)
                        mx = int((x1 + x2) / 2 * self.display_scale)
                        my = int((y1 + y2) / 2 * self.display_scale)
                        painter.setPen(QPen(color, 1))
                        painter.drawText(mx + 6, my - 6, cls)
                        if gps and isinstance(gps, list) and len(gps) >= 2:
                            if gps[0] is not None and isinstance(gps[0], (list, tuple)) and len(gps[0]) == 2:
                                painter.setPen(QPen(color, 1))
                                painter.drawText(sx1 + 8, sy1 + 12, f"{gps[0][0]:.6f}, {gps[0][1]:.6f}")
                            if gps[1] is not None and isinstance(gps[1], (list, tuple)) and len(gps[1]) == 2:
                                painter.setPen(QPen(color, 1))
                                painter.drawText(sx2 + 8, sy2 + 12, f"{gps[1][0]:.6f}, {gps[1][1]:.6f}")
                    elif kind == "curve":
                        path = item
                        for k, (px, py) in enumerate(path):
                            sx, sy = int(px * self.display_scale), int(py * self.display_scale)
                            is_hover = self.hover == (kind, cls, i, k)
                            pen = QPen(QColor("yellow") if is_hover else color, 4)
                            painter.setPen(pen)
                            painter.drawEllipse(QPoint(sx, sy), 4, 4)
                        for k in range(1, len(path)):
                            x1, y1 = [int(p * self.display_scale) for p in path[k - 1]]
                            x2, y2 = [int(p * self.display_scale) for p in path[k]]
                            painter.setPen(QPen(color, 2))
                            painter.drawLine(QPoint(x1, y1), QPoint(x2, y2))
                        if path:
                            mx = sum(p[0] for p in path) / len(path)
                            my = sum(p[1] for p in path) / len(path)
                            painter.setPen(QPen(color, 1))
                            painter.drawText(int(mx * self.display_scale + 6),
                                             int(my * self.display_scale - 6), cls)
        if len(self.current_line) == 1:
            cx, cy = [int(p * self.display_scale) for p in self.current_line[0]]
            painter.setPen(QPen(QColor("blue"), 2))
            painter.drawEllipse(QPoint(cx, cy), 5, 5)
        if self.current_curve:
            painter.setPen(QPen(QColor("blue"), 2))
            for k in range(1, len(self.current_curve)):
                x1, y1 = [int(p * self.display_scale) for p in self.current_curve[k - 1]]
                x2, y2 = [int(p * self.display_scale) for p in self.current_curve[k]]
                painter.drawLine(QPoint(x1, y1), QPoint(x2, y2))
            for (px, py) in self.current_curve:
                sx, sy = int(px * self.display_scale), int(py * self.display_scale)
                painter.drawEllipse(QPoint(sx, sy), 4, 4)
        painter.end()
        self.image_label.setPixmap(pix)
    def save_annotations(self):
        path, _ = QFileDialog.getSaveFileName(self, "Сохранить аннотации", "", "JSON (*.json)")
        if not path:
            return
        with open(path, "w") as f:
            json.dump(self.annotations, f, indent=2)
    def load_annotations(self):
        path, _ = QFileDialog.getOpenFileName(self, "Загрузить аннотации", "", "JSON (*.json)")
        if not path:
            return
        with open(path, "r") as f:
            self.annotations = json.load(f)
        for key in ["point", "line", "curve"]:
            if key not in self.annotations:
                self.annotations[key] = {}
        for cls in list(self.annotations["curve"].keys()):
            valid = []
            for ann in self.annotations["curve"][cls]:
                if (
                        isinstance(ann, dict) and
                        "image" in ann and
                        isinstance(ann["image"], list) and
                        len(ann["image"]) >= 2 and
                        all(isinstance(pt, (list, tuple)) and len(pt) == 2 for pt in ann["image"])
                ):
                    valid.append(ann)
            if valid:
                self.annotations["curve"][cls] = valid
            else:
                del self.annotations["curve"][cls]
        self.update_display()
        self.class_selector.clear()
        self.class_selector.addItem("all")
        known_classes = set()
        for kind in ["point", "line", "curve"]:
            for cls in self.annotations.get(kind, {}).keys():
                known_classes.add(cls)
        for cls in sorted(known_classes):
            self.class_selector.addItem(cls)
    def add_class(self):
        cls = self.class_selector.currentText().strip()
        if cls and self.class_selector.findText(cls) == -1:
            self.class_selector.addItem(cls)
        self.class_selector.setCurrentText(cls)
if __name__ == "__main__":
    app = QApplication(sys.argv)
    tool = AnnotationTool()
    tool.show()
    sys.exit(app.exec_())
# ==== source\annotation_tools\data_preparation.py ====
import numpy as np
import os
import json
from core.pointND import PointND
def load_data(path):
    lines = []
    with open(path, 'r') as file:
        for line in file:
            name, cords = line.split(':')
            points = eval(cords.strip())
            lines.append([PointND([x, y]) for x, y in points])
    return lines
def prep_data_angle(data):
    _data = []
    if len(data) % 2 == 0:
        for i in range(0, len(data), 2):
            _data.append(data[i] + data[i + 1])
        return np.array(_data)
    else:
        raise ValueError("Кол-во линий не четное число")
def prep_data_parallel(data):
    _data = []
    for i in range(0, len(data) - 1):
        _data.append(data[i] + data[i + 1])
    return np.array(_data)
def load_params(path):
    with open(path, 'r') as file:
        return [float(value) for value in file.readline().split()]
def prep_data_back_to_reverse(camera, data):
    data = np.array(data)
    data_calc = []
    for start, end in data:
        start_3d = camera.back_crop(start)
        end_3d = camera.back_crop(end)
        data_calc.append([camera.direct_crop(start_3d), camera.direct_crop(end_3d)])
    return np.array(data_calc)
def fun_lines(x, start: PointND, end: PointND, orthogonal=False):
    x1, y1 = start.get()
    x2, y2 = end.get()
    if not orthogonal:
        return (x - x1) * (y2 - y1) / (x2 - x1) + y1
    else:
        m = (y2 - y1) / (x2 - x1)
        return (-1 / m) * (x - x1) + y1
def load_lines(filename):
    if not os.path.exists(filename):
        print("Файл аннотаций не найден.")
        return
    with open(filename, "r") as f:
        data = json.load(f)
    lines = [[tuple(point) for point in line] for line in data.values()]
    return lines
import json
def load_lines_from_json(filepath: str):
    with open(filepath, 'r', encoding='utf-8') as f:
        data = json.load(f)
    lines = []
    for item in data:
        gps_start = item['start']['gps']
        gps_end = item['end']['gps']
        pix_start = item['start']['pixel']
        pix_end = item['end']['pixel']
        line = {
            'gps': [gps_start, gps_end],  # [[lat1, lon1], [lat2, lon2]]
            'pixel': [pix_start, pix_end],  # [[x1, y1], [x2, y2]]
        }
        lines.append(line)
    return lines
# ==== source\calibration\__init__.py ====
from .base import Calibration
from .pipeline import CalibrationPipeline
from .refine.optimizer import RefineOptimizer
from .init.from_vp import VanishingPointCalibration
__all__ = [
    "Calibration",
    "CalibrationPipeline",
    "RefineOptimizer",
    "VanishingPointCalibration"
]
__version__ = "0.1.0"
__author__ = "Акмурзин Миша"
# ==== source\calibration\base.py ====
from abc import ABC, abstractmethod
import numpy as np
from source.core.camera import Camera
RESUALDS = {}
i = 0
class Calibration(ABC):
    def __init__(self, camera: Camera = None, debug_save_path: str = None):
        self.camera = camera
        self.debug_save_path = debug_save_path
    @abstractmethod
    def run(self, data: dict, **kwargs) -> Camera:
        pass
    def compute_total_residuals(self, camera, data, params, residual_blocks):
        global i
        camera.set_params_from_list(params)
        residuals = []
        data_residuals = {}
        for block in residual_blocks:
            res, group = block(camera, data)
            residuals.extend(res)
            data_residuals[group] = res
        i += 1
        RESUALDS[i] = data_residuals
        return np.array(residuals)
    def compute_total_mse(self, camera, data, params, residual_blocks):
        camera.set_params_from_list(params)
        residuals = 0
        for block in residual_blocks:
            res = block(camera, data)
            residuals += res
        print(residuals)
        return residuals
# ==== source\calibration\pipeline.py ====
from source.core.camera import Camera
from .base import Calibration
class CalibrationPipeline:
    def __init__(self, init_stage=None, refine_stages: list = None, n_iter: int = 1):
        """
        :param init_stage: начальная калибровка (обычно по точкам схода)
        :param refine_stages: список уточняющих оптимизаторов (по координатам и т.п.)
        :param n_iter: количество итераций уточняющих этапов
        """
        self.init_stage = init_stage
        self.refine_stages = refine_stages or []
        self.n_iter = n_iter
    def run(self, camera: Camera, data: dict, **kwargs) -> Camera:
        """
        Запуск начального этапа и итераций уточняющих оптимизаций.
        :param camera: объект Camera
        :param data: словарь с разметкой
        :param kwargs: дополнительные параметры
        :return: откалиброванная камера
        """
        if self.init_stage:
            print(f"🚀 [Pipeline] Начальный этап: {self.init_stage.__class__.__name__}")
            self.init_stage.camera = camera
            camera = self.init_stage.run(data, **kwargs)
        for iteration in range(1, self.n_iter + 1):
            print(f"🔁 [Pipeline] Итерация уточнения {iteration}/{self.n_iter}")
            for idx, stage in enumerate(self.refine_stages, 1):
                stage.camera = camera
                print(f"🔧 [Pipeline] Этап {idx}: {stage.__class__.__name__}")
                camera = stage.run(data, **kwargs)
        print("🎯 [Pipeline] Калибровка завершена")
        print("=" * 60)
        print(f"[Pipeline] Конечные значения {[round(float(p), 2) for p in camera.get_params()]}")
        return camera
# ==== source\calibration\debug\__init__.py ====
from .debug_vp import visualize_vps_debug
from .debug_scene import visualize_grid_debug, load_scene_gps, visualize_grid_gps_debug, set_grid_real
from .debug_source import visualize_source
from .debug_metrics import estimate_rotation_svd, generate_yandex_maps_url, compute_alignment_and_metrics
from .debud_projection import projection_line
from .debug_refine import plot_residuals_comparison
__all__ = [
    "visualize_vps_debug",
    "visualize_grid_debug",
    "visualize_grid_gps_debug",
    "visualize_source",
    "load_scene_gps",
    "set_grid_real",
    "estimate_rotation_svd",
    "generate_yandex_maps_url",
    "compute_alignment_and_metrics",
    "plot_residuals_comparison"
    ]
# ==== source\calibration\debug\debud_projection.py ====
import numpy as np
import matplotlib.pyplot as plt
from core import Camera, PointND
from calibration.utils import gps_to_enu
def projection_line(camera, data, lat0, lon0, save_path, R=np.array([[1, 0], [0, 1]])):
    image = camera.get_image()
    fig, ax = plt.subplots(figsize=(12, 8))
    ax.imshow(image)
    annotation_plotted = False
    projected_plotted = False
    for line in data:
        p1, p2 = line['pixel']
        P1_gps, P2_gps = line['gps']
        P1, P2 = [
            camera.project_direct(PointND([*R @ gps_to_enu(*point, lat0, lon0), 0])).get()
            for point in line['gps']
        ]
        label_ann = 'Исходные линии' if not annotation_plotted else None
        label_proj = 'Спроецированные линии' if not projected_plotted else None
        ax.plot([p1[0], p2[0]], [p1[1], p2[1]], 'r-', label=label_ann)
        ax.plot([P1[0], P2[0]], [P1[1], P2[1]], 'b--', label=label_proj)
        ax.plot(*p1, 'ro')
        ax.plot(*p2, 'ro')
        ax.plot(*P1, 'bo')
        ax.plot(*P2, 'bo')
        annotation_plotted = True
        projected_plotted = True
    ax.set_title("Сравнение аннотаций и проекций")
    ax.legend(loc='lower right')
    ax.axis('off')
    fig.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close(fig)
# ==== source\calibration\debug\debug_metrics.py ====
import numpy as np
from source.calibration.utils import gps_to_enu, enu_to_gps
from source.core import PointND
def generate_yandex_maps_url(points):
    base_url = "https://yandex.ru/maps/?pt="
    coords = ["{:.6f},{:.6f}".format(lon, lat) for lat, lon in points]
    return base_url + "~".join(coords)
def estimate_rotation_svd(points_cam, points_enu):
    """
    points_cam — Nx2 точки из камеры (в мировой системе XY)
    points_enu — Nx2 точки в ENU системе координат
    """
    A = np.array(points_cam)
    B = np.array(points_enu)
    A_mean = A.mean(axis=0)
    B_mean = B.mean(axis=0)
    A_centered = A - A_mean
    B_centered = B - B_mean
    H = A_centered.T @ B_centered
    U, S, Vt = np.linalg.svd(H)
    R = Vt.T @ U.T
    if np.linalg.det(R) < 0:
        Vt[1, :] *= -1
        R = Vt.T @ U.T
    return R
def compute_alignment_and_metrics(
        point_image,  # Точки с изображения
        point_gps_ideal,  # Идеальные GPS точки
        lat0, lon0,  # Начальная точка ENU-системы
        camera,  # Объект камеры
        save_path="yandex_comparison.html"
):
    points_cam = [
        camera.project_back(PointND(pt, add_weight=True)).get()[:2]
        for pt in point_image
    ]
    points_enu = [
        gps_to_enu(lat, lon, lat0, lon0)
        for lat, lon in point_gps_ideal
    ]
    R = estimate_rotation_svd(points_cam, points_enu)
    errors = [
        np.linalg.norm(
            np.array( predict) -
            np.array(ideal)
        )
        for predict, ideal in zip(points_cam, points_enu)
    ]
    stats = {
        "Средняя ошибка": np.mean(errors),
        "Стандартное отклонение": np.std(errors),
        "Минимальная ошибка": np.min(errors),
        "Максимальная ошибка": np.max(errors),
        "Медианная ошибка": np.median(errors),
    }
    print("\n📊 Статистика ошибок (в метрах):")
    for name, value in stats.items():
        print(f"  ▸ {name:<24} {value:.2f} м")
    point_gps_predict = [enu_to_gps(*point, lat0, lon0) for point in points_cam]
    save_yandex_comparison_map_html(point_gps_ideal, point_gps_predict, save_path)
    return {
        "rotation_matrix": R,
        "errors": errors,
        "stats": stats,
        "point_gps_predict": point_gps_predict,
        "point_gps_ideal": point_gps_ideal,
    }
def save_yandex_comparison_map_html(point_gps_ideal, point_gps_predict, save_path):
    """
    Сохраняет HTML-файл с Яндекс.Картой, на которой отображаются:
    - Зелёные точки: идеальные GPS координаты
    - Синие точки: предсказанные GPS (после обратной проекции)
    - Красные линии: векторы ошибок
    """
    if len(point_gps_ideal) != len(point_gps_predict):
        raise ValueError("Количество точек должно совпадать")
    center_lat = sum(lat for lat, lon in point_gps_ideal) / len(point_gps_ideal)
    center_lon = sum(lon for lat, lon in point_gps_ideal) / len(point_gps_ideal)
    placemarks = ""
    polylines = ""
    for i, (ideal, pred) in enumerate(zip(point_gps_ideal, point_gps_predict)):
        lat1, lon1 = ideal
        lat2, lon2 = pred
        placemarks += f"""
        myMap.geoObjects.add(new ymaps.Placemark([{lat1}, {lon1}], {{
            balloonContent: "📍 Исходная точка {i + 1}",
            hintContent: "📍 Исходная точка {i + 1}"
        }}, {{
            preset: "islands#greenDotIcon"
        }}));
        myMap.geoObjects.add(new ymaps.Placemark([{lat2}, {lon2}], {{
            balloonContent: "🎯 Спроецированная точка {i + 1}",
            hintContent: "🎯 Спроецированная точка {i + 1}"
        }}, {{
            preset: "islands#blueDotIcon"
        }}));
        """
        polylines += f"""
        myMap.geoObjects.add(new ymaps.Polyline([
            [{lat1}, {lon1}],
            [{lat2}, {lon2}]
        ], {{
            hintContent: "Вектор ошибки"
        }}, {{
            strokeColor: "#FF0000",
            strokeWidth: 3,
            strokeOpacity: 0.7
        }}));
        """
    html = f"""<!DOCTYPE html>
<html>
<head>
    <title>Сравнение GPS точек</title>
    <meta charset="utf-8" />
    <script src="https://api-maps.yandex.ru/2.1/?lang=ru_RU" type="text/javascript"></script>
</head>
<body>
<div id="map" style="width: 100%; height: 600px;"></div>
<script>
ymaps.ready(function () {{
    var myMap = new ymaps.Map("map", {{
        center: [{center_lat}, {center_lon}],
        zoom: 18
    }});
    {placemarks}
    {polylines}
}});
</script>
</body>
</html>"""
    with open(save_path, "w", encoding="utf-8") as f:
        f.write(html)
    print(f"✅ HTML-карта с точками и векторами ошибок сохранена в: {save_path}")
# ==== source\calibration\debug\debug_position_camera.py ====
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
from sympy.physics.units import length
from core import Camera, PointND
def visualize_camera_and_world_axes(
        camera: Camera
):
    """
    Отрисовка системы координат камеры относительно мировой системы координат
    """
    R = camera.extrinsics.get_rotation().T
    C = camera.extrinsics.get_position()
    x_cam = R[:, 0]  # вправо
    y_cam = R[:, 1]  # вверх
    z_cam = R[:, 2]  # взгляд
    scale = 10
    fig = plt.figure(figsize=(8, 8))
    ax = fig.add_subplot(111, projection='3d')
    ax.quiver(0, 0, 0, 1, 0, 0, length=scale, color='red', label='World X')
    ax.quiver(0, 0, 0, 0, 1, 0, length=scale, color='green', label='World Y')
    ax.quiver(0, 0, 0, 0, 0, 1, length=scale, color='blue', label='World Z')
    ax.quiver(*C, *x_cam, length=scale, color='r', linestyle='dashed', label='Camera X')
    ax.quiver(*C, *y_cam, length=scale, color='g', linestyle='dashed', label='Camera Y')
    ax.quiver(*C, *z_cam, length=scale, color='b', linestyle='dashed', label='Camera Z')
    ax.scatter(*C, label='C')
    ax.scatter(*-R @ C, label='- R @ C')
    point_start = PointND([960, 540])
    plane_z = 0
    grid_range = 10
    grid_step = 1
    anchor_3D = camera.project_back(point_start, plane_z=plane_z)
    anchor_x, anchor_y, anchor_z = anchor_3D.get()
    to_scene = anchor_3D.get() - C
    if np.dot(R[:, 2], to_scene) < 0:
        print("Камера смотрит в обратную сторону — Z нужно инвертировать")
    count = int(2 * grid_range / grid_step) + 1
    world_points = []
    for i in range(count):
        for j in range(count):
            x = anchor_x - grid_range + i * grid_step
            y = anchor_y - grid_range + j * grid_step
            world_points.append(PointND(np.array([x, y, plane_z])))
    for i in range(count):
        for j in range(count - 1):
            p1 = world_points[i * count + j].get()
            p2 = world_points[i * count + (j + 1)].get()
            ax.plot([p1[0], p2[0]], [p1[1], p2[1]], color='blue', linewidth=1)
    for j in range(count):
        for i in range(count - 1):
            p1 = world_points[i * count + j].get()
            p2 = world_points[(i + 1) * count + j].get()
            ax.plot([p1[0], p2[0]], [p1[1], p2[1]], color='blue', linewidth=1)
    ax.set_xlabel('X')
    ax.set_ylabel('Y')
    ax.set_zlabel('Z')
    ax.set_title('Камера и мировая система координат')
    ax.legend()
    plt.tight_layout()
    plt.show()
camera = Camera("../../../example/pushkin_aksakov/image/pattern_corrected_image.png")
camera.set_params_from_list([1230, -13.46, 48.12, -164.54, 0, 0, 10])
visualize_camera_and_world_axes(camera)
# ==== source\calibration\debug\debug_refine.py ====
import matplotlib.pyplot as plt
import numpy as np
def plot_residuals_comparison(RESUALDS):
    """
    Строит график сравнения остатков на первом и последнем шаге оптимизации.
    RESUALDS — словарь вида: {step_num: {'metric_name': [values], ...}, ...}
    """
    fig, ax = plt.subplots(figsize=(14, 5))
    step_start = 1
    step_end = max(RESUALDS)
    x_offset = 0
    xticks = []
    xticklabels = []
    keys = list(RESUALDS[step_start].keys())
    def short_name(name):
        """
        Сокращает длинные метки автоматически:
        - оставляет первые буквы слов
        - убирает повторяющиеся шаблоны
        """
        name = name.replace("Расстояние", "R")
        name = name.replace("точки", "T")
        name = name.replace("в", "V")
        name = name.replace("до", "D")
        name = name.replace("линии", "L")
        return ''.join(word[0].upper() for word in name.split() if word)
    for i, key in enumerate(keys):
        y_start = RESUALDS[step_start][key]
        y_end = RESUALDS[step_end][key]
        n = len(y_start)
        x_range = np.arange(n) + x_offset
        ax.plot(x_range, y_start, 'o-', label=f'{key} (step 1)')
        ax.plot(x_range, y_end, 's--', label=f'{key} (step last)')
        short_key = short_name(key)
        xticks.extend(x_range)
        xticklabels.extend([f'{short_key}{j}' for j in range(n)])
        if i < len(keys) - 1:
            ax.axvline(x_range[-1] + 0.5, color='gray', linestyle='--')
        x_offset += n
    ax.set_xticks(xticks)
    ax.set_xticklabels(xticklabels, rotation=90, fontsize=8)
    ax.set_ylabel("Residual value")
    ax.set_title("Сравнение остатков на первом и последнем шаге")
    ax.legend(fontsize=8)
    ax.grid(True)
    plt.tight_layout()
    plt.show()
# ==== source\calibration\debug\debug_scene.py ====
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.lines as mlines
from pyproj import Proj, transform
import requests
from PIL import Image
from io import BytesIO
from source.core import Camera, PointND
from source.calibration.utils import enu_to_gps
def visualize_grid_debug(
        camera: Camera,
        point_start: PointND,
        grid_range_x: float = 5.0,  # диапазон по X (влево/вправо от центра)
        grid_range_y: float = 5.0,  # диапазон по Y (вперёд/назад от центра)
        grid_step: float = 1.0,     # размер клетки
        arrow_len: float = 5,    # длина вектора "вверх"
        plane_z: float = 0.0,
        save_path=None
):
    image = camera.get_image()
    height, width = image.shape[:2]
    fig, ax = plt.subplots(figsize=(12, 7))
    ax.set_title("Калибровочная сетка (1×1 м) в проекции")
    ax.set_xlim(0, width)
    ax.set_ylim(height, 0)
    plt.imshow(image)
    plt.scatter(*point_start.get(), c='red')
    anchor_3D = camera.project_back(point_start, plane_z=plane_z)
    anchor_x, anchor_y, anchor_z = anchor_3D.get()
    count_x = int(2 * grid_range_x / grid_step) + 1
    count_y = int(2 * grid_range_y / grid_step) + 1
    world_points = []
    for j in range(count_y):
        for i in range(count_x):
            x = anchor_x - grid_range_x + i * grid_step
            y = anchor_y - grid_range_y + j * grid_step
            world_points.append(PointND(np.array([x, y, plane_z])))
    for j in range(count_y):
        for i in range(count_x - 1):
            idx1 = j * count_x + i
            idx2 = j * count_x + (i + 1)
            p1_proj = camera.project_direct(world_points[idx1]).get()
            p2_proj = camera.project_direct(world_points[idx2]).get()
            ax.plot([p1_proj[0], p2_proj[0]], [p1_proj[1], p2_proj[1]], color='blue', linewidth=1)
    for i in range(count_x):
        for j in range(count_y - 1):
            idx1 = j * count_x + i
            idx2 = (j + 1) * count_x + i
            p1_proj = camera.project_direct(world_points[idx1]).get()
            p2_proj = camera.project_direct(world_points[idx2]).get()
            ax.plot([p1_proj[0], p2_proj[0]], [p1_proj[1], p2_proj[1]], color='blue', linewidth=1)
    def draw_arrow_from_3D(p3D):
        base = camera.project_direct(p3D).get()
        tip_point = PointND(p3D.get() + np.array([0, 0, arrow_len]))
        tip = camera.project_direct(tip_point).get()
        ax.scatter(base[0], base[1], color='red', s=10, zorder=3)
        ax.annotate(
            '', xy=(tip[0], tip[1]), xytext=(base[0], base[1]),
            arrowprops=dict(arrowstyle='->', color='black', lw=1.5),
            annotation_clip=False
        )
    top_left = world_points[0]
    top_right = world_points[count_x - 1]
    bottom_left = world_points[(count_y - 1) * count_x]
    bottom_right = world_points[-1]
    for corner in [top_left, top_right, bottom_left, bottom_right]:
        draw_arrow_from_3D(corner)
    draw_coordinate_system_overlay(camera, ax, scale=10)
    arrow_legend = mlines.Line2D([], [], color='black', marker=r'$\uparrow$', linestyle='None',
                                 markersize=10, label=f'Вектор вверх ({arrow_len} м)')
    ax.legend(handles=[arrow_legend])
    if save_path is not None:
        plt.savefig(save_path)
def set_grid_real(anchor_x, anchor_y, grid_range, grid_step, plane_z):
    count = int(2 * grid_range / grid_step) + 1
    world_points = []
    for i in range(count):
        for j in range(count):
            x = anchor_x - grid_range + i * grid_step
            y = anchor_y - grid_range + j * grid_step
            world_points.append(PointND(np.array([x, y, plane_z])))
    return count, world_points
def draw_coordinate_system_overlay(camera: Camera, ax, scale=10):
    origin = PointND([0, 0, 0, 1])
    X = PointND([scale, 0, 0, 1])
    Y = PointND([0, scale, 0, 1])
    Z = PointND([0, 0, scale, 1])
    p0 = camera.project_direct(origin).get()
    print(f'Пиксель мировой системы координат: {p0}')
    px = camera.project_direct(X).get()
    py = camera.project_direct(Y).get()
    pz = camera.project_direct(Z).get()
    def draw_arrow(p_start, p_end, color, label):
        ax.annotate('', xy=p_end[:2], xytext=p_start[:2],
                    arrowprops=dict(arrowstyle='->', linewidth=2, color=color))
        ax.text(p_end[0], p_end[1], label, color=color,
                fontsize=12, fontweight='bold', ha='center', va='center')
    draw_arrow(p0, px, 'red', 'X')
    draw_arrow(p0, py, 'green', 'Y')
    draw_arrow(p0, pz, 'blue', 'Z')
def visualize_coordinate_system(camera: Camera, save_path: str):
    image = camera.get_image()
    scale = 10  # длина осей в условных единицах
    origin = PointND([0, 0, 0, 1])
    X = PointND([scale, 0, 0, 1])
    Y = PointND([0, scale, 0, 1])
    Z = PointND([0, 0, scale, 1])
    p0 = camera.project_direct(origin).get()
    px = camera.project_direct(X).get()
    py = camera.project_direct(Y).get()
    pz = camera.project_direct(Z).get()
    fig, ax = plt.subplots(figsize=(10, 10))
    ax.imshow(image)
    ax.axis('off')
    def draw_arrow(p_start, p_end, color, label):
        ax.annotate(
            '', xy=p_end[:2], xytext=p_start[:2],
            arrowprops=dict(arrowstyle='->', linewidth=2, color=color)
        )
        ax.text(p_end[0], p_end[1], label, color=color,
                fontsize=12, fontweight='bold', ha='center', va='center')
    draw_arrow(p0, px, 'red', 'X')
    draw_arrow(p0, py, 'green', 'Y')
    draw_arrow(p0, pz, 'blue', 'Z')
    plt.savefig(save_path, bbox_inches='tight', pad_inches=0)
def load_scene_gps(lon, lat, save_path=None, zoom=19, size=(650, 450)):
    url = f"https://static-maps.yandex.ru/1.x/?ll={lon},{lat}&z={zoom}&l=sat&size={size[0]},{size[1]}"
    response = requests.get(url)
    image = Image.open(BytesIO(response.content))
    if save_path is not None:
        image.save(save_path)
    return image
def gps_to_pixel(lat, lon, ref_lat, ref_lon, img_width, img_height, meters_per_pixel=0.3):
    """
    Преобразует GPS в пиксели, предполагая, что ref_lat/lon находится в центре.
    """
    from pyproj import Geod
    geod = Geod(ellps="WGS84")
    def meters_per_pixel(zoom, lat):
        return 156543.03392 * np.cos(np.deg2rad(lat)) / (2 ** zoom)
    az_east, _, east = geod.inv(ref_lon, ref_lat, lon, ref_lat)
    az_north, _, north = geod.inv(ref_lon, ref_lat, ref_lon, lat)
    if lat < ref_lat:
        north = -north
    if lon < ref_lon:
        east = -east
    x = img_width / 2 + east / meters_per_pixel
    y = img_height / 2 - north / meters_per_pixel  # сверху вниз
    return x, y
def visualize_grid_gps_debug(
        camera: Camera,
        point_start: PointND,
        gps_origin: tuple,
        grid_range: float = 10.0,  # диапазон в метрах от центра
        grid_step: float = 1.0,  # размер клетки
        plane_z: float = 0.0,  # плоскость, на которую кладём сетку
        save_path=None
):
    ref_lat, ref_lon = gps_origin
    image = load_scene_gps(ref_lon, ref_lat, zoom=19)
    image_np = np.array(image)
    height, width = image_np.shape[:2]
    fig, ax = plt.subplots(figsize=(10, 10))
    ax.imshow(image_np)
    ax.set_title("ENU-сетка на спутнике")
    anchor_3D = camera.project_back(point_start, plane_z=plane_z)
    anchor_x, anchor_y, anchor_z = anchor_3D.get()
    count, world_points = set_grid_real(anchor_x, anchor_y, grid_range, grid_step, plane_z)  # enu
    for i in range(count):
        for j in range(count - 1):
            world_point_1 = world_points[i * count + j]
            world_point_2 = world_points[i * count + j + 1]
            east1, north1 = world_point_1.get()[:2]
            east2, north2 = world_point_2.get()[:2]
            lat1, lon1 = enu_to_gps(east1, north1, ref_lat, ref_lon)
            lat2, lon2 = enu_to_gps(east2, north2, ref_lat, ref_lon)
            px1, py1 = gps_to_pixel(lat1, lon1, ref_lat, ref_lon, width, height)
            px2, py2 = gps_to_pixel(lat2, lon2, ref_lat, ref_lon, width, height)
            ax.plot([px1, px2], [py1, py2], color='red')
    for j in range(count):
        for i in range(count - 1):
            world_point_1 = world_points[i * count + j]
            world_point_2 = world_points[(i + 1) * count + j]
            east1, north1 = world_point_1.get()[:2]
            east2, north2 = world_point_2.get()[:2]
            lat1, lon1 = enu_to_gps(east1, north1, ref_lat, ref_lon)
            lat2, lon2 = enu_to_gps(east2, north2, ref_lat, ref_lon)
            px1, py1 = gps_to_pixel(lat1, lon1, ref_lat, ref_lon, width, height)
            px2, py2 = gps_to_pixel(lat2, lon2, ref_lat, ref_lon, width, height)
            ax.plot([px1, px2], [py1, py2], color='red')
    plt.show()
# ==== source\calibration\debug\debug_source.py ====
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import hashlib
import numpy as np
def visualize_source(data: dict, image=None):
    """
    data: {
        "group1": [ [(x1,y1), (x2,y2)], ... ],
        "group2": [ [(x1,y1), (x2,y2)], ... ],
    }
    image: optional background image (e.g. from camera.get_image())
    """
    fig, ax = plt.subplots()
    if image is not None:
        ax.imshow(image)
    for key, lines in data.items():
        color = get_color_by_key(key)
        for i, (p1, p2) in enumerate(lines):
            ax.plot([p1[0], p2[0]], [p1[1], p2[1]], color=color, linewidth=2)
            ax.scatter(*p1, color=color, s=10)
            ax.scatter(*p2, color=color, s=10)
        ax.plot([], [], color=color, label=key)
    ax.legend(loc='upper right')
    ax.axis('equal')
    plt.tight_layout()
    plt.show()
def get_color_by_key(key):
    """Уникальный цвет по имени группы"""
    cmap = cm.get_cmap('tab10')
    hash_val = int(hashlib.md5(key.encode()).hexdigest(), 16)
    return cmap(hash_val % 10)
# ==== source\calibration\debug\debug_vp.py ====
import matplotlib.pyplot as plt
import numpy as np
import cv2
def draw_coordinate_axes_from_vps(
        vanishing_points,
        center,
        scale=100,
        labels=None,
        colors=None,
        flip_z=True,
        image=None,
        save_path=None,
        ax=None
):
    if labels is None:
        labels = ['X', 'Y', 'Z']
    if colors is None:
        colors = ['red', 'green', 'blue']
    cx, cy = center
    if ax is None:
        fig, ax = plt.subplots()
    if image is not None:
        ax.imshow(image)
    for i, (x, y) in enumerate(vanishing_points):
        dx = x - cx
        dy = y - cy
        norm = np.hypot(dx, dy)
        dx_scaled = dx / norm * scale
        dy_scaled = dy / norm * scale
        if flip_z and labels[i].upper() == 'Z':
            dx_scaled *= -1
            dy_scaled *= -1
        ax.arrow(cx, cy, dx_scaled, dy_scaled,
                 color=colors[i], width=1.2, head_width=10, length_includes_head=True)
        ax.text(cx + dx_scaled * 1.1, cy + dy_scaled * 1.1,
                labels[i], fontsize=12, color=colors[i], weight='bold')
    ax.set_title("Coordinate Axes from Vanishing Points")
    ax.axis('off')
    if save_path:
        plt.savefig(save_path, dpi=150)
def visualize_vps_debug(
        camera,
        step_x=400,
        step_y=300,
        scale=100,
        save_path=None,
        show=False,
        flip_z=True,
        dpi=200
):
    """
    Визуализирует координатные оси на изображении камеры по точкам схода.
    :param camera: объект камеры с методом get_image(), intrinsics.get(), extrinsics.get_rotation()
    :param step_x: шаг сетки по оси X (в пикселях)
    :param step_y: шаг сетки по оси Y (в пикселях)
    :param scale: длина координатных стрелок
    :param save_path: путь для сохранения изображения
    :param show: показывать ли окно с результатом
    :param flip_z: отражать ли ось Z
    :param dpi: разрешение сохранённого изображения
    """
    image = camera.get_image()
    K = camera.intrinsics.get()
    R = camera.extrinsics.get_rotation()
    vp1 = K @ R[:, 0]
    vp2 = K @ R[:, 1]
    vp3 = K @ R[:, 2]
    vp1 = vp1[:2] / vp1[2]
    vp2 = vp2[:2] / vp2[2]
    vp3 = vp3[:2] / vp3[2]
    vps = np.array([vp1, vp2, vp3])
    h, w = image.shape[:2]
    centers = [
        (int(x), int(y))
        for y in np.arange(step_y, h, step_y)
        for x in np.arange(step_x, w, step_x)
    ]
    fig, ax = plt.subplots(figsize=(12, 8))
    ax.imshow(image)
    for center in centers:
        draw_coordinate_axes_from_vps(
            vanishing_points=vps,
            center=center,
            scale=scale,
            flip_z=flip_z,
            ax=ax
        )
    ax.set_title("Vanishing Point Coordinate Axes")
    ax.axis('off')
    plt.tight_layout()
    if save_path:
        plt.savefig(save_path, dpi=dpi, bbox_inches='tight')
        print(f"[DEBUG] Saved visualization to {save_path}")
    if show:
        plt.show()
    else:
        plt.close(fig)
# ==== source\calibration\init\__init__.py ====
from .from_vp import VanishingPointCalibration
__all__ = [
    "VanishingPointCalibration"
]# ==== source\calibration\init\from_vp.py ====
import numpy as np
from ..base import Calibration
from source.core import Camera, PointND
class VanishingPointCalibration(Calibration):
    def __init__(self, camera: Camera, debug_save_path: str = None):
        super().__init__(camera, debug_save_path)
        self.vpX = None  # точка схода по оси X (горизонт)
        self.vpY = None  # точка схода по оси Y (горизонт)
        self.vpZ = None  # точка схода по оси Z (вертикаль)
    def set_vanishing_points(self, vpX, vpY=None, vpZ=None):
        self.vpX = np.array(vpX, dtype=float)
        if vpY is not None:
            self.vpY = np.array(vpY, dtype=float)
        if vpZ is not None:
            self.vpZ = np.array(vpZ, dtype=float)
    def calc_f(self):
        cx, cy = self.camera.intrinsics.get_main_point()
        c = np.array([cx, cy, 1.0])
        if self.vpX is not None and self.vpZ is not None:
            v1 = np.append(self.vpX, 1.0)
            v2 = np.append(self.vpZ, 1.0)
            term = np.dot(v1 - c, c - v2)
            if term <= 0:
                raise ValueError("Подкоренное выражение отрицательно. Проверь точки схода.")
            f = np.sqrt(term)
            return f
        elif self.vpX is not None and self.vpY is not None:
            v1 = np.append(self.vpX, 1.0)
            v2 = np.append(self.vpY, 1.0)
            term = np.dot(v1 - c, c - v2)
            if term <= 0:
                raise ValueError("Подкоренное выражение отрицательно. Проверь точки схода.")
            f = np.sqrt(term)
            return f
    def calc_R(self, f):
        self.camera.intrinsics.set_focal_length(f)
        K_inv = np.linalg.inv(self.camera.intrinsics.get())
        dx = K_inv @ np.append(self.vpX, 1.0)
        dy = K_inv @ np.append(self.vpY, 1.0) if self.vpY is not None else None
        dz = K_inv @ np.append(self.vpZ, 1.0) if self.vpZ is not None else None
        return self._build_rotation(dx, dy, dz)
    def _build_rotation(self, dx, dy, dz):
        x = dx / np.linalg.norm(dx)
        if dy is not None and dz is not None:
            y = dy / np.linalg.norm(dy)
            z = dz / np.linalg.norm(dz)
            z = z - np.dot(z, x) * x - np.dot(z, y) * y
            z /= np.linalg.norm(z)
            y = np.cross(z, x)
            y /= np.linalg.norm(y)
        elif dy is None:
            z = dz / np.linalg.norm(dz)
            y = np.cross(z, x)
            y /= np.linalg.norm(y)
        elif dz is None:
            y = dy / np.linalg.norm(dy)
            z = np.cross(x, y)
            z /= np.linalg.norm(z)
        else:
            raise ValueError("В сцене только одна точка схода. Проверь точки схода.")
        R = np.column_stack((x, y, z))
        z = R[:, 2]
        if z[2] > 0:  # например, камера "смотрит вверх" — это неправильно
            print("[VP Init] ⚠️ Камера смотрит назад — инвертируем ориентацию")
            R[:, 2] *= -1
            R[:, 1] = np.cross(R[:, 2], R[:, 0])
            R[:, 1] /= np.linalg.norm(R[:, 1])
        print(f' [VP Init] Determinant(R): {np.linalg.det(R)}')
        return R
    def run(self, data=None, **kwargs):
        """
        Выполняет инициализацию параметров камеры по точкам схода.
        :return: обновлённая камера
        """
        print("=" * 50)
        print("🎯 [VP Init] Инициализация параметров по точкам схода")
        print("=" * 50)
        f = self.calc_f()
        print(f"🔬 Вычислено фокусное расстояние: f = {f:.4f}")
        R = self.calc_R(f)
        self.camera.extrinsics.set_rotation(R, from_type='vp')
        angles = self.camera.extrinsics.get_angles()
        print(f"🧭 Углы ориентации (ZXY, град): {np.round(angles, 2)}")
        print("✅ [VP Init] Инициализация завершена")
        if self.debug_save_path is not None:
            from source.calibration.debug import visualize_vps_debug
            print(f"💾 Сохраняю визуализацию в: {self.debug_save_path}")
            visualize_vps_debug(self.camera, save_path=self.debug_save_path)
        print("=" * 50)
        return self.camera
# ==== source\calibration\refine\__init__.py ====
from .optimizer import RefineOptimizer
from .back_error_funk import residual_interline_distance, residual_parallel_group, residual_line_length, \
    residual_planar_alignment, residual_vertical_alignment, residual_alignment_block, residual_orthogonality_error
from .direct_error_funk import residual_reprojection_line, residual_reprojection_point
from .refine_angle import get_plane_normal
__all__ = [
    "RefineOptimizer",
    "residual_interline_distance",
    "residual_parallel_group",
    "residual_reprojection_line",
    "residual_line_length",
    "residual_planar_alignment",
    "residual_vertical_alignment",
    "residual_alignment_block",
    "get_plane_normal",
    "residual_reprojection_point",
    "residual_orthogonality_error"
]
# ==== source\calibration\refine\back_error_funk.py ====
import numpy as np
from scipy.spatial.transform import Rotation as R
from source.core import Camera, PointND
def residual_interline_distance(camera, data, group, expected):
    residuals = []
    lines = data.get(group, [])
    for i in range(len(lines) -  1):
        d = compute_interline_distance(camera, lines[i], lines[i + 1])
        residuals.append(np.abs(d - expected))
    return residuals
def compute_interline_distance(camera: Camera, line1, line2, plane_z=0):
    c1 = PointND(np.mean(line1, axis=0))
    c2 = PointND(np.mean(line2, axis=0))
    X1 = camera.project_back(c1, plane_z).get()
    X2 = camera.project_back(c2, plane_z).get()
    P1a = camera.project_back(PointND(line1[0]), plane_z).get()
    P1b = camera.project_back(PointND(line1[1]), plane_z).get()
    direction = P1b - P1a
    direction = direction[:2] / np.linalg.norm(direction[:2])
    normal = np.array([-direction[1], direction[0]])  # ортогонально в 2D
    delta = (X2 - X1)[:2]
    dist = np.abs(np.dot(delta, normal))
    return dist
def residual_parallel_group(camera, data, group, plane_z=0):
    """
    Резидуал: проверяет, что все линии в группе параллельны в мировой системе координат.
    Ошибка = косое произведение направляющих векторов.
    :param camera: модель камеры
    :param data: словарь с линиями
    :param group: имя ключа в data
    :param plane_z: плоскость обратной проекции
    :return: список residuals
    """
    residuals = []
    lines = data.get(group, [])
    if len(lines) < 2:
        return residuals  # нечего сравнивать
    directions = []
    for p1, p2 in lines:
        X1 = camera.project_back(PointND(p1), plane_z).get()
        X2 = camera.project_back(PointND(p2), plane_z).get()
        d = X2 - X1
        norm = np.linalg.norm(d)
        if norm < 1e-6:
            continue  # вырожденная
        directions.append(d[:2] / norm)  # только XY-плоскость
    if len(directions) < 2:
        return residuals  # не хватает направлений
    ref = directions[0]
    for d in directions[1:]:
        cross = np.cross(ref, d)  # → 0, если параллельны
        residuals.append(cross)
    return residuals
def compute_line_length(camera: Camera, line, plane_z=0):
    """
    Возвращает длину линии в 3D, восстановленной из двух концов.
    """
    P1 = camera.project_back(PointND(line[0]), plane_z).get()
    P2 = camera.project_back(PointND(line[1]), plane_z).get()
    return np.linalg.norm(P2 - P1)
def residual_line_length(camera, data, group, expected):
    """
    Возвращает список остатков (residuals), включающий:
    - отклонения расстояний между линиями от expected_spacing
    - отклонения длин линий от expected_length
    """
    residuals = []
    lines = data.get(group, [])
    for line in lines:
        L = compute_line_length(camera, line, 0)
        residuals.append(np.abs(L - expected))
    return residuals
def residual_orthogonality_error(camera, data, group):
    residuals = []
    lines = data.get(group, [])
    for (p1, p2), (q1, q2) in lines:
        p1_world = camera.project_back(PointND(p1)).get()
        p2_world = camera.project_back(PointND(p2)).get()
        q1_world = camera.project_back(PointND(q1)).get()
        q2_world = camera.project_back(PointND(q2)).get()
        v1 = p2_world - p1_world
        v2 = q2_world - q1_world
        v1 /= np.linalg.norm(v1)
        v2 /= np.linalg.norm(v2)
        dot = np.dot(v1, v2)
        residuals.append(dot ** 2)
    return residuals
def residual_planar_alignment(omega, R0, K, planar_lines_img):
    delta_R = R.from_rotvec(omega).as_matrix()
    R_corr = delta_R @ R0
    K_inv = np.linalg.inv(K)
    residuals = []
    for line_dir in planar_lines_img:
        line_dir = line_dir / np.linalg.norm(line_dir)
        dir_img_h = np.array([line_dir[0], line_dir[1], 1.0])
        dir_cam = K_inv @ dir_img_h
        dir_cam = dir_cam / np.linalg.norm(dir_cam)
        dir_world = R_corr.T @ dir_cam
        z_component = dir_world[2]
        residuals.append(z_component)  # просто сама компонента, без квадрата
    return residuals
def residual_vertical_alignment(omega, R0, K, lines_img):
    delta_R = R.from_rotvec(omega).as_matrix()
    R_corr = delta_R @ R0
    z_world = np.array([0, 0, 1])
    residuals = []
    for line_dir in lines_img:
        line_dir = line_dir / np.linalg.norm(line_dir)
        v_cam = R_corr @ z_world
        v_img = K @ v_cam
        v_img = v_img[:2] / v_img[2]
        v_img = v_img / np.linalg.norm(v_img)
        cos_theta = np.dot(v_img, line_dir)
        residuals.append(1 - cos_theta ** 2)
    return residuals
def residual_alignment_block(verticals=None, planar_lines=None, weights=(1.0, 1.0, 10.0)):
    """
    Возвращает residual-функцию совместимую с RefineOptimizer:
    (camera, data) -> List[float]
    """
    def block(camera, data):
        angles_current = camera.get_params()[1:4]
        R_current = R.from_euler('zyx', angles_current, degrees=True)
        if not hasattr(block, "_R0"):
            block._R0 = R_current
            block._angles0 = angles_current
        R0 = block._R0.as_matrix()
        R1 = R_current.as_matrix()
        R_delta = R1 @ R0.T
        omega = R.from_matrix(R_delta).as_rotvec()
        residuals = []
        if verticals:
            res_vert = residual_vertical_alignment(omega, R0, camera.intrinsics.get(), verticals)
            residuals.extend([weights[0] * r for r in res_vert])
        if planar_lines:
            res_planar = residual_planar_alignment(omega, R0, camera.intrinsics.get(), planar_lines)
            residuals.extend([weights[1] * r for r in res_planar])
        residuals.extend([weights[2] * w for w in omega])
        return residuals
    return block
# ==== source\calibration\refine\direct_error_funk.py ====
import numpy as np
from source.core import Camera, PointND
from source.calibration.utils import gps_to_enu
def residual_reprojection_line(camera, data, group, gps_origin=None):
    residuals = []
    lines = data.get(group, [])
    for line in lines:
        if gps_origin is not None:
            p1, p2 = line['pixel']
            P1, P2 = line['gps']
            _p1 = camera.project_direct(PointND([*gps_to_enu(*P1, *gps_origin), 0])).get()
            _p2 = camera.project_direct(PointND([*gps_to_enu(*P2, *gps_origin), 0])).get()
            error1 = np.sum((np.array(_p2) - np.array(p2)) ** 2)
            error2 = np.sum((np.array(_p1) - np.array(p1)) ** 2)
            error = np.sqrt(error1 + error2)
            residuals.append(error)
        else:
            p1, p2 = line['pixel']
            P1, P2 = line['gps']
            _p1 = camera.project_direct([*P1]).get()
            _p2 = camera.project_direct([*P2]).get()
            error1 = np.sum((np.array(_p2) - np.array(p2)) ** 2)
            error2 = np.sum((np.array(_p1) - np.array(p1)) ** 2)
            error = np.sqrt(error1 + error2)
            residuals.append(error)
    return residuals
def residual_reprojection_point(camera, data, group, gps_origin=None):
    residuals = []
    points = data.get(group, [])
    for point in points:
        if gps_origin is not None:
            p1 = point['pixel']
            P1 = point['gps']
            _p1 = camera.project_direct(PointND([*gps_to_enu(*P1, *gps_origin), 0])).get()
            error2 = np.sum((np.array(_p1) - np.array(p1)) ** 2)
            error = np.sqrt(error2)
            residuals.append(error)
        else:
            p1 = point['pixel']
            P1 = point['gps']
            _p1 = camera.project_direct(PointND([*P1])).get()
            error2 = np.sum((np.array(_p1) - np.array(p1)) ** 2)
            error = np.sqrt(error2)
            residuals.append(error)
    return residuals
def line_projection_error(camera: Camera, line, gps_origin):
    p1, p2 = line['pixel']
    P1, P2 = line['gps']
    P1, P2 = gps_to_enu(*gps_origin, *P1)
    return 0
def point_projection_error(camera: Camera, point) -> float:
    point2D, point3D = point
    proj2D = camera.project_direct(point3D)
    return np.linalg.norm(point2D.get() - proj2D.get())
# ==== source\calibration\refine\optimizer.py ====
import numpy as np
from pandas.core.methods.selectn import SelectNSeries
from scipy.optimize import least_squares, minimize
from scipy.spatial.transform import Rotation as R
from source.calibration.base import Calibration
from source.core.camera import Camera
from source.core.pointND import PointND
class RefineOptimizer(Calibration):
    def __init__(self, camera: Camera,
                 residual_blocks: list,
                 bounds: tuple = None,
                 solver=least_squares,
                 method: str = "trf",
                 mask: list = None,
                 debug_save_path: str = None,
                 gps_origin: tuple = None,
                 omega_mode=False,
                 grid_range=(10, 10),
                 point_start=None
                 ):
        super().__init__(camera, debug_save_path)
        self.residual_blocks = residual_blocks
        self.bounds = bounds if bounds is not None else ([800, -360, -360, -360, -30, -30, 5],
                                                         [2000, 360, 360, 360, 30, 30, 30])
        self.mask = mask if mask is not None else [0, 1, 2, 3, 4, 5, 6]
        self.solver = solver
        self.method = method
        self.gps_origin = gps_origin
        self.omega_mode = omega_mode
        self.grid_range_x, self.grid_range_y = grid_range
        self.initial_params = camera.get_params()
        self.point_start = point_start if point_start is not None else self.camera.intrinsics.get_main_point()
    def run(self, data, **kwargs):
        if self.omega_mode:
            return self._run_omega_mode(data)
        else:
            return self._run_normal(data, **kwargs)
    def _run_normal(self, data, **kwargs) -> Camera:
        """
        :param data: ограничения
        :return: обновлённая камера
        """
        full_params = np.array(self.camera.get_params(), dtype=float)
        print(f"📌 Начальные параметры: {np.round(full_params, 2).tolist()}")
        x0 = full_params[self.mask]
        def loss_fn(masked_params):
            current_params = full_params.copy()
            current_params[self.mask] = masked_params
            return self.compute_total_residuals(self.camera, data, current_params, self.residual_blocks)
        def loss_fn_mse(masked_params):
            current_params = full_params.copy()
            current_params[self.mask] = masked_params
            return self.compute_total_mse(self.camera, data, current_params, self.residual_blocks)
        if self.method == "lm":
            result = self.solver(loss_fn,
                                 x0,
                                 method=self.method,
                                 max_nfev=10000
                                 )
        elif self.method == "trf":
            result = self.solver(loss_fn,
                                 x0,
                                 method=self.method,
                                 bounds=self.bounds,
                                 max_nfev=3000,
                                 gtol=1e-8,
                                 xtol=1e-8,
                                 ftol=1e-8
                                 )
        elif self.method == "minimize":
            result = minimize(
                fun=loss_fn_mse,
                x0=x0,
                bounds=self.bounds,
                method="Powell"
            )
        print(f"🔁 Итераций: {result.nfev}")
        if self.method in ["trf", "lm"]:
            print(f"🎯 Финальная ошибка (cost): {result.cost:.6f}")
        else:
            print(f"🎯 Финальная ошибка (cost): {result.fun:.6f}")
        print("📍 Обновлённые параметры:", np.round(result.x, 2).tolist())
        full_params[self.mask] = result.x
        self.camera.set_params_from_list(full_params)
        if self.debug_save_path is not None:
            from source.calibration.debug import visualize_grid_debug, visualize_grid_gps_debug
            point_start = PointND(self.point_start, add_weight=True)
            visualize_grid_debug(self.camera, point_start, save_path=self.debug_save_path, grid_range_x=self.grid_range_x,
                                 grid_range_y=self.grid_range_y,
                                 grid_step=1)
            if self.gps_origin is not None:
                pass
        return self.camera
    def _run_omega_mode(self, data):
        scaled_omega0 = np.zeros(2)  # только ω_x и ω_y
        R0 = self.camera.extrinsics.get_rotation()
        K = self.camera.intrinsics.get()
        scale = 0.01
        def cost_fn(scaled_omega_xy):
            omega = np.array([scaled_omega_xy[0], scaled_omega_xy[1], 0]) * scale
            delta_R = R.from_rotvec(omega).as_matrix()
            self.camera.extrinsics.set_rotation(delta_R @ R0, from_type='vp')
            residuals = []
            for block in self.residual_blocks:
                residuals.extend(block(self.camera, data))
            reg_weight = 400
            regularization = reg_weight * np.sum(omega[:2] ** 2)
            return np.sum(np.square(residuals)) + regularization
        result = minimize(cost_fn, scaled_omega0, method='BFGS')
        omega_opt = np.array([result.x[0], result.x[1], 0.0]) * scale
        delta_R = R.from_rotvec(omega_opt).as_matrix()
        self.camera.extrinsics.set_rotation(delta_R @ R0, from_type='vp')
        return self.camera
# ==== source\calibration\refine\refine_angle.py ====
import numpy as np
from core.camera import Camera
def get_plane_normal(camera, p1, p2):
    r1 = camera.backproject_ray(p1)  # (3,)
    r2 = camera.backproject_ray(p2)  # (3,)
    normal = np.cross(r1, r2)
    return normal / np.linalg.norm(normal)
# ==== source\calibration\utils\__init__.py ====
from  .data_preparation import load_lines, load_lines_from_json, \
    extract_direction_vectors_from_lines
from .gps_connection_world import gps_to_enu, enu_to_gps
__all__ = [
    "gps_to_enu",
    "enu_to_gps",
    "load_lines_from_json",
    "load_lines",
    "extract_direction_vectors_from_lines",
]
# ==== source\calibration\utils\data_preparation.py ====
import numpy as np
import os
import json
from source.core.pointND import PointND
def load_data(path):
    lines = []
    with open(path, 'r') as file:
        for line in file:
            name, cords = line.split(':')
            points = eval(cords.strip())
            lines.append([PointND([x, y]) for x, y in points])
    return lines
def prep_data_angle(data):
    _data = []
    if len(data) % 2 == 0:
        for i in range(0, len(data), 2):
            _data.append(data[i] + data[i + 1])
        return np.array(_data)
    else:
        raise ValueError("Кол-во линий не четное число")
def prep_data_parallel(data):
    _data = []
    for i in range(0, len(data) - 1):
        _data.append(data[i] + data[i + 1])
    return np.array(_data)
def load_params(path):
    with open(path, 'r') as file:
        return [float(value) for value in file.readline().split()]
def prep_data_back_to_reverse(camera, data):
    data = np.array(data)
    data_calc = []
    for start, end in data:
        start_3d = camera.back_crop(start)
        end_3d = camera.back_crop(end)
        data_calc.append([camera.direct_crop(start_3d), camera.direct_crop(end_3d)])
    return np.array(data_calc)
def fun_lines(x, start: PointND, end: PointND, orthogonal=False):
    x1, y1 = start.get()
    x2, y2 = end.get()
    if not orthogonal:
        return (x - x1) * (y2 - y1) / (x2 - x1) + y1
    else:
        m = (y2 - y1) / (x2 - x1)
        return (-1 / m) * (x - x1) + y1
def load_lines(filename):
    if not os.path.exists(filename):
        print("Файл аннотаций не найден.")
        return
    with open(filename, "r") as f:
        data = json.load(f)
    lines = [[tuple(point) for point in line] for line in data.values()]
    return lines
import json
def load_lines_from_json(filepath: str):
    with open(filepath, 'r', encoding='utf-8') as f:
        data = json.load(f)
    lines = []
    for item in data:
        gps_start = item['start']['gps']
        gps_end = item['end']['gps']
        pix_start = item['start']['pixel']
        pix_end = item['end']['pixel']
        line = {
            'gps': [gps_start, gps_end],  # [[lat1, lon1], [lat2, lon2]]
            'pixel': [pix_start, pix_end],  # [[x1, y1], [x2, y2]]
        }
        lines.append(line)
    return lines
def extract_direction_vectors_from_lines(lines):
    """
    Преобразует список линий, заданных парами точек [(x1, y1), (x2, y2)],
    в нормированные направляющие векторы [dx, dy].
    """
    direction_vectors = []
    for (x1, y1), (x2, y2) in lines:
        dx = x2 - x1
        dy = y2 - y1
        direction = np.array([dx, dy], dtype=np.float64)
        norm = np.linalg.norm(direction)
        if norm > 1e-6:  # проверка на нулевую длину
            direction_vectors.append(direction / norm)
    return direction_vectors# ==== source\calibration\utils\gps_connection_world.py ====
import numpy as np
from pyproj import Geod
from source.core import PointND, Camera
def gps_to_enu(lat, lon, ref_lat, ref_lon):
    """
    Перевод GPS (широта, долгота) в локальные координаты ENU (в метрах)
    """
    geod = Geod(ellps="WGS84")
    azimuth, _, distance = geod.inv(ref_lon, ref_lat, lon, lat)
    east = distance * np.sin(np.deg2rad(azimuth))
    north = distance * np.cos(np.deg2rad(azimuth))
    return east, north
def enu_to_gps(east, north, ref_lat, ref_lon):
    """
    Преобразует локальные координаты ENU (в метрах)
    обратно в GPS-координаты (широта, долгота).
    Parameters:
    east : float
        Смещение на восток (в метрах)
    north : float
        Смещение на север (в метрах)
    ref_lat : float
        Начальная широта (градусы)
    ref_lon : float
        Начальная долгота (градусы)
    Returns:
    lat : float
        Новая широта
    lon : float
        Новая долгота
    """
    geod = Geod(ellps="WGS84")
    distance = np.hypot(east, north)
    azimuth = np.rad2deg(np.arctan2(east, north))
    lon, lat, _ = geod.fwd(ref_lon, ref_lat, azimuth, distance)
    return lat, lon
def compute_alignment_rotation(camera, image_point, gps0, gps1):
    """
    Вычисляет матрицу поворота, которая совмещает направление из камеры (через project_back)
    с направлением на основе двух GPS-точек.
    camera: объект камеры
    image_point: пиксель на изображении (u, v)
    gps0, gps1: (lat, lon) — задают направление в ENU
    Возвращает:
    - матрицу поворота 2×2 (numpy.array)
    - угол в радианах
    """
    v_scene_all = camera.project_back(PointND(image_point, add_weight=True)).get()[:2]
    v_scene = v_scene_all / np.linalg.norm(v_scene_all)
    v_enu_all = gps_to_enu(gps1[0], gps1[1], gps0[0], gps0[1])
    v_enu = v_enu_all / np.linalg.norm(v_enu_all)
    cos_theta = np.dot(v_scene, v_enu)
    sin_theta = v_scene[0] * v_enu[1] - v_scene[1] * v_enu[0]
    theta = np.arctan2(sin_theta, cos_theta)
    R = np.array([
        [np.cos(theta), -np.sin(theta)],
        [np.sin(theta), np.cos(theta)]
    ])
    return R, theta
# ==== source\core\__init__.py ====
from .camera import Camera
from .pointND import PointND
from .camera_intrinsics import CameraIntrinsics
from .camera_extrinsics import CameraExtrinsics
__all__ = [
    "Camera",
    "PointND",
    "CameraIntrinsics",
    "CameraExtrinsics"
]
__version__ = "0.1.0"
__author__ = "Акмурзин Миша"# ==== source\core\camera.py ====
import cv2
import numpy as np
from .camera_intrinsics import CameraIntrinsics
from .camera_extrinsics import CameraExtrinsics
from .pointND import PointND
class Camera:
    def __init__(self, path_image=None, size=None):
        if path_image is not None:
            self.image = cv2.cvtColor(cv2.imread(path_image), cv2.COLOR_BGR2RGB)
            self.size = self.image.shape[:2]  # (height, width)
            self.path = path_image
        elif size is not None:
            self.image = None
            self.size = size  # size = (height, width)
            self.path = None
        else:
            raise ValueError("Нужно указать либо path_image, либо size")
        self.intrinsics = CameraIntrinsics(self.size[1], self.size[0])
        self.extrinsics = CameraExtrinsics()
    def set_params(self, params: dict):
        """
        Устанавливает параметры камеры из словаря.
        Ожидаемые ключи:
            - f или (fx, fy): фокусное расстояние (одно или два)
            - rz, rx, ry: углы Эйлера в градусах (если from_type='euler')
            - vp: список из трёх направлений (если from_type='vp')
            - x, y, z: координаты положения камеры
            - from_type: 'euler' или 'vp'
        """
        if "f" in params:
            self.intrinsics.set_focal_length(params["f"])
        elif "fx" in params and "fy" in params:
            self.intrinsics.set_focal_length((params["fx"], params["fy"]))
        else:
            raise ValueError("Отсутствует фокусное расстояние (f или fx/fy)")
        from_type = params.get("from_type", "euler")
        if from_type == "euler":
            angles = [params.get("rz", 0), params.get("rx", 0), params.get("ry", 0)]
            self.extrinsics.set_rotation(angles, from_type="euler")
        elif from_type == "vp":
            vp_list = params.get("vp")
            if vp_list is None or len(vp_list) != 3:
                raise ValueError("Для from_type='vp' нужно три вектора vp")
            self.extrinsics.set_rotation(vp_list, from_type="vp")
        else:
            raise ValueError(f"Неизвестный тип from_type: {from_type}")
        x = params.get("x", 0)
        y = params.get("y", 0)
        z = params.get("z", 0)
        self.extrinsics.set_position(x, y, z)
    def set_params_from_list(self, param_list: list):
        """
        Устанавливает параметры камеры из плоского списка.
        Ожидается формат: [f, rz, rx, ry, x, y, z]
        """
        if len(param_list) != 7:
            raise ValueError("Ожидается 7 параметров: f, rz, rx, ry, x, y, z")
        f = param_list[0]
        rz, rx, ry = param_list[1:4]
        x, y, z = param_list[4:7]
        self.intrinsics.set_focal_length(f)
        self.extrinsics.set_rotation([rz, rx, ry], from_type="euler")
        self.extrinsics.set_position(x, y, z)
    def get_params(self) -> list:
        params = []
        f = self.intrinsics.get_focal_length()
        if isinstance(f, tuple):  # fx, fy
            params.extend(f)
        else:
            params.append(f)
        rz, rx, ry = self.extrinsics.get_angles()
        params.extend([rz, rx, ry])
        x, y, z = self.extrinsics.get_position()
        assert isinstance(x, float)
        params.extend([x, y, z])
        return params
    def get_image(self):
        return self.image
    def get_size(self):
        return self.size
    def project_direct(self, point3D: PointND) -> PointND:
        RT = self.extrinsics.get()
        K = self.intrinsics.get()
        P = K @ RT
        point2D = PointND(P @ point3D.get(out_homogeneous=True), add_weight=False)
        return point2D
    def project_back(self, point2D: PointND, plane_z: float = 0.0) -> PointND:
        K = self.intrinsics.get()
        R = self.extrinsics.get_rotation()
        C = np.array(self.extrinsics.get_position())
        x = point2D.get(out_homogeneous=True)
        K_inv = np.linalg.inv(K)
        ray_cam = K_inv @ x  # направление в системе камеры
        ray_world = R.T @ ray_cam  # в мировой системе координат
        ray_world = ray_world / np.linalg.norm(ray_world)
        t = (plane_z - C[2]) / ray_world[2]  # ищем такой t, чтобы Z == plane_z
        point3D = C + t * ray_world  # точка на плоскости
        return PointND(point3D, add_weight=True)
    def backproject_ray(self, pixel_2d: tuple[float, float]) -> np.ndarray:
        """
        Возвращает направляющий вектор луча, проходящего от центра камеры через пиксель.
        В мировой системе координат.
        """
        K = self.intrinsics.get()
        R = self.extrinsics.get_rotation()
        x = np.array([pixel_2d[0], pixel_2d[1], 1.0])
        ray_cam = np.linalg.inv(K) @ x
        ray_world = R.T @ ray_cam
        ray_world /= np.linalg.norm(ray_world)
        return ray_world
def homography(self, point: PointND, direction='direct') -> PointND:
    RT = self.extrinsics.get()
    RT = np.delete(RT, 2, axis=1)  # удаляем третий столбец (оси Z) ⇒ проекция на плоскость Z=0
    H = self.intrinsics.get() @ RT  # Гомография
    p = point.get(out_homogeneous=True)
    if direction == 'direct':
        transformed = H @ p
    elif direction == 'back':
        H_inv = np.linalg.inv(H)
        transformed = H_inv @ p
    else:
        raise ValueError("Аргумент direction должен быть 'direct' или 'back'.")
    return PointND(transformed, add_weight=False)
# ==== source\core\camera_extrinsics.py ====
import numpy as np
from scipy.spatial.transform import Rotation
class CameraExtrinsics:
    def __init__(self):
        self.R = np.eye(3)
        self.C = np.array([0, 0, 15])
    def set_rotation(self, data, from_type='euler'):
        if from_type == 'euler':
            self.R = Rotation.from_euler('zxy', data, degrees=True).as_matrix()
        elif from_type == 'vp':
            if data.shape == (3, 3):
                self.R = data
            else:
                raise ValueError("Ожидается матрица 3x3 для from_type='vp'")
        else:
            raise ValueError("Неверный тип ориентации")
    def set_position(self, x=0, y=0, z=10):
        self.C = np.array([x, y, z])
    def get_rotation(self):
        return self.R
    def get_angles(self, order='zxy', degrees=True):
        """
        :return: (rz, rx, ry)
        """
        return Rotation.from_matrix(self.R).as_euler(order, degrees=degrees)
    def get_position(self):
        return tuple(float(c) for c in np.ravel(self.C))
    def get(self):
        t = -self.R @ self.C
        RT = np.hstack([self.R, t.reshape(3, 1)])
        return RT
# ==== source\core\camera_intrinsics.py ====
import numpy as np
class CameraIntrinsics:
    def __init__(self, width, height):
        self.width = width
        self.height = height
        self.fx = None
        self.fy = None
        self.K = np.eye(3)
    def set_focal_length(self, f):
        self.fx, self.fy = (f, f) if not isinstance(f, (tuple, list)) else f
        self.K = np.array([[self.fx, 0, self.width / 2],
                           [0, self.fy, self.height / 2],
                           [0, 0, 1]])
    def get(self):
        return self.K
    def get_main_point(self):
        return self.width / 2, self.height / 2
    def get_focal_length(self):
        if self.fx == self.fy:
            return self.fx
        else:
            return (self.fx, self.fy)
# ==== source\core\pointND.py ====
import numpy as np
class PointND:
    def __init__(self, coord, add_weight=True):
        coord = np.asarray(coord)
        if len(coord) + 1 in [3, 4] and add_weight:
            coord = np.append(coord, 1)
        self.coord = coord
    def set(self, coord):
        self.coord = np.append(coord, 1) if len(coord) + 1 == len(self.coord) else coord
    def get(self, out_homogeneous=False):
        return self.coord if out_homogeneous else self.coord[:-1] / self.coord[-1]
    def get_type(self):
        dim = len(self.coord) - 1
        return f"{dim}D"
    def set_Z(self, z):
        if len(self.coord) > 3:
            self.coord[2] = z
        else:
            raise ValueError("Объект не является 3D точкой")
# ==== source\distortion\__init__.py ====
# ==== source\distortion\recovery_image_1_params.py ====
import numpy as np
import cv2
def compute_distorted_r(r_vals, k1):
    r2 = r_vals ** 2
    L = 1 / (1 + k1 * r2)
    return L * r_vals
from scipy.interpolate import interp1d
def build_inverse_vector(k1, r_max, num_samples=10000):
    r_vals = np.linspace(0, r_max, num_samples)
    r_distorted = compute_distorted_r(r_vals, k1)
    mask = r_vals > 0
    scale = np.max(r_distorted[mask] / r_vals[mask])
    r_distorted, indices = np.unique(r_distorted, return_index=True)
    r_vals = r_vals[indices]
    inverse_func = interp1d(r_distorted, r_vals, bounds_error=False, fill_value="extrapolate")
    print(f'scale {scale}')
    return inverse_func, scale  # функция: r_hat → r
def build_undistort_map(img_shape, k1, cx, cy, inverse_func):
    h, w = img_shape[:2]
    map_x = np.zeros((h, w), dtype=np.float32)
    map_y = np.zeros((h, w), dtype=np.float32)
    for j in range(h):
        for i in range(w):
            dx = i - cx
            dy = j - cy
            r_hat = np.sqrt(dx ** 2 + dy ** 2)
            if r_hat == 0:
                map_x[j, i] = i
                map_y[j, i] = j
                continue
            r = inverse_func(r_hat)
            scale = r / r_hat
            x = cx + dx * scale
            y = cy + dy * scale
            map_x[j, i] = x
            map_y[j, i] = y
    return map_x, map_y
def build_undistort_map_vec(img_shape, k1, cx, cy, inverse_func, scale=1.0):
    h, w = img_shape[:2]
    x, y = np.meshgrid(np.arange(w), np.arange(h))
    dx = (x - cx) * scale
    dy = (y - cy) * scale
    r_hat = np.sqrt(dx ** 2 + dy ** 2)
    r = inverse_func(r_hat.ravel()).reshape(r_hat.shape)
    scale = np.ones_like(r_hat)
    valid = r_hat > 0
    scale[valid] = r[valid] / r_hat[valid]
    map_x = (cx + dx * scale).astype(np.float32)
    map_y = (cy + dy * scale).astype(np.float32)
    return map_x, map_y
def undistort_image(img, k1):
    h, w = img.shape[:2]
    cx, cy = w / 2, h / 2
    r_max = np.sqrt(cx ** 2 + cy ** 2)
    inverse_func, scale = build_inverse_vector(k1, r_max)
    map_x, map_y = build_undistort_map_vec(img.shape, k1, cx, cy, inverse_func, scale=scale)
    print("min/max map_x:", np.min(map_x), np.max(map_x))
    print("min/max map_y:", np.min(map_y), np.max(map_y))
    undistorted = cv2.remap(img, map_x, map_y, interpolation=cv2.INTER_LINEAR)
    return undistorted
img = cv2.imread("...")
k1 = -3.6645511758874987309e-07
result = undistort_image(img, k1)
cv2.imwrite("undistorted_output_one.jpg", result)
# ==== source\distortion\recovery_image_2_params.py ====
import numpy as np
import cv2
def compute_distorted_r(r_vals, k1, k2):
    r2 = r_vals ** 2
    L = 1 + k1 * r2 + k2 * r2 ** 2
    return L * r_vals
from scipy.interpolate import interp1d
def build_inverse_vector(k1, k2, r_max, num_samples=10000):
    r_vals = np.linspace(0, r_max, num_samples)
    r_distorted = compute_distorted_r(r_vals, k1, k2)
    mask = r_vals > 0
    scale = np.max(r_distorted[mask] / r_vals[mask])
    r_distorted, indices = np.unique(r_distorted, return_index=True)
    r_vals = r_vals[indices]
    inverse_func = interp1d(r_distorted, r_vals, bounds_error=False, fill_value="extrapolate")
    print(f'scale {scale}')
    return inverse_func, scale  # функция: r_hat → r
def build_undistort_map(img_shape, k1, k2, cx, cy, inverse_func):
    h, w = img_shape[:2]
    map_x = np.zeros((h, w), dtype=np.float32)
    map_y = np.zeros((h, w), dtype=np.float32)
    for j in range(h):
        for i in range(w):
            dx = i - cx
            dy = j - cy
            r_hat = np.sqrt(dx ** 2 + dy ** 2)
            if r_hat == 0:
                map_x[j, i] = i
                map_y[j, i] = j
                continue
            r = inverse_func(r_hat)
            scale = r / r_hat
            x = cx + dx * scale
            y = cy + dy * scale
            map_x[j, i] = x
            map_y[j, i] = y
    return map_x, map_y
def build_undistort_map_vec(img_shape, k1, k2, cx, cy, inverse_func, scale=1.0):
    h, w = img_shape[:2]
    x, y = np.meshgrid(np.arange(w), np.arange(h))
    dx = (x - cx) * scale
    dy = (y - cy) * scale
    r_hat = np.sqrt(dx ** 2 + dy ** 2)
    r = inverse_func(r_hat.ravel()).reshape(r_hat.shape)
    scale = np.ones_like(r_hat)
    valid = r_hat > 0
    scale[valid] = r[valid] / r_hat[valid]
    map_x = (cx + dx * scale).astype(np.float32)
    map_y = (cy + dy * scale).astype(np.float32)
    return map_x, map_y
def undistort_image(img, k1, k2):
    h, w = img.shape[:2]
    cx, cy = w / 2, h / 2
    r_max = np.sqrt(cx ** 2 + cy ** 2)
    inverse_func, scale = build_inverse_vector(k1, k2, r_max)
    map_x, map_y = build_undistort_map_vec(img.shape, k1, k2, cx, cy, inverse_func, scale=scale)
    print("min/max map_x:", np.min(map_x), np.max(map_x))
    print("min/max map_y:", np.min(map_y), np.max(map_y))
    undistorted = cv2.remap(img, map_x, map_y, interpolation=cv2.INTER_LINEAR)
    return undistorted
img = cv2.imread("crossroads.jpg")
k1, k2 = 1.640657049166558e-07, 2.2242158526886968e-13
result = undistort_image(img, k1, k2)
cv2.imwrite("undistorted_output.jpg", result)
# ==== source\distortion\search_params_1.py ====
import matplotlib.pyplot as plt
import sys
import os
module_dir = r"D:\Final qualifying work\Main\source\distortion\cmake-build-debug\extended_hough_transform"
sys.path.insert(0, module_dir)
import ami
import imageio.v3 as iio
import numpy as np
from PIL import Image
img_path = "test/screenshot_1749312396624.jpg"
img = Image.open(img_path)
img.save("test/crossroads.png")
img_path = "test/crossroads.png"
image = iio.imread(img_path)
height, width = image.shape[:2]
points = ami.run_canny(img_path, 0.5, 0.7)
xs, ys, dxs, dys = zip(*points) if points else ([], [], [], [])
res = ami.run_hough(
    xs, ys, dxs, dys,
    width=width, height=height,
    distance_point_line_max=3.0,
    max_lines=30,
    initial_distortion_param=0,
    final_distortion_param=2,
    angle_point_orientation_max_diff=3
)
print("Best param:", res["best_param"])
plt.imshow(res["accumulator"], cmap='hot')
plt.title("Accumulator")
plt.colorbar()
plt.show()
fig, ax = plt.subplots(figsize=(10, 8))
ax.imshow(image)
for line in res["lines"]:
    a, b, c = line["a"], line["b"], line["c"]
    x_vals = np.array([0, image.shape[1]])
    if abs(b) > 1e-6:
        y_vals = -(a * x_vals + c) / b
    else:
        x_vals = np.full(2, -c / a) if abs(a) > 1e-6 else np.array([0, 0])
        y_vals = np.array([0, image.shape[0]])
    ax.plot(x_vals, y_vals, 'r-', linewidth=1.5)
for line in res["lines"]:
    for x, y in line["points"]:
        ax.plot(x, y, 'go', markersize=2)
ax.set_title(f"Best distortion param: {res['best_param']:.4f}")
plt.axis("off")
plt.tight_layout()
plt.show()
# ==== source\distortion\search_params_2.py ====
import numpy as np
import cv2
from scipy.optimize import minimize
import matplotlib.pyplot as plt
def compute_r2(image_shape):
    h, w = image_shape[:2]
    r1 = np.sqrt((w / 2) ** 2 + (h / 2) ** 2)
    return r1 / 2
def denormalize_p_to_k(p1, p2, r2):
    k1 = (p1 - 16 * p2) / (-12 * r2 ** 2)
    k2 = (4 * p2 - p1) / (-12 * r2 ** 4)
    return k1, k2
def undistort_point(xy, k1, k2, cx, cy):
    """
    Применяет радиальную дисторсию к точкам в изображении.
    xy — массив точек (N, 2)
    cx, cy — центр дисторсии (обычно центр изображения)
    """
    dx = xy[:, 0] - cx
    dy = xy[:, 1] - cy
    r2 = dx ** 2 + dy ** 2
    L = 1 + k1 * r2 + k2 * r2 ** 2
    x_corr = cx + dx * L
    y_corr = cy + dy * L
    return np.stack([x_corr, y_corr], axis=1)
def curve_residuals(curves_undistorted):
    total_error = 0
    for curve in curves_undistorted:
        if len(curve) < 2:
            continue
        curve = np.array(curve)
        fit = np.polyfit(curve[:, 0], curve[:, 1], 1)
        y_fit = np.polyval(fit, curve[:, 0])
        error = np.mean((curve[:, 1] - y_fit) ** 2)
        total_error += error
    return total_error
def objective(params, curves, image_shape, r2):
    p1, p2 = params
    k1, k2 = denormalize_p_to_k(p1, p2, r2)
    global cx, cy
    cx, cy = image_shape[1] / 2, image_shape[0] / 2
    undistorted_curves = []
    for curve in curves:
        pts = np.array(curve)
        undistorted = undistort_point(pts, k1, k2, cx, cy)
        undistorted_curves.append(undistorted)
    return curve_residuals(undistorted_curves)
curves = np.load("clicked_curves.npy", allow_pickle=True)[:-1]
image_shape = (1080, 1920)
r2 = compute_r2(image_shape)
result = minimize(
    fun=objective,
    x0=np.array([0.0, 0.0]),
    args=(curves, image_shape, r2),
    method="Nelder-Mead",
    bounds=[(-1.0, 1.0), (-1.0, 1.0)]
)
print(result)
print(result.x, cx, cy)
opt_p1, opt_p2 = result.x
opt_k1, opt_k2 = denormalize_p_to_k(opt_p1, opt_p2, r2)
print(opt_k1, opt_k2)
# ==== source\vp_detection\__init__.py ====
from .vanishing_point_estimator import VanishingPointEstimatorManual
__all__ = [
    "VanishingPointEstimatorManual",
]
# ==== source\vp_detection\base.py ====
from abc import ABC, abstractmethod
class Detector(ABC):
    def run(self, **kwargs):
        pass
# ==== source\vp_detection\vanishing_point_estimator.py ====
import numpy as np
from .base import Detector
class VanishingPointEstimatorManual(Detector):
    def __init__(self):
        pass
    @staticmethod
    def _normal_vector(x1, y1, x2, y2):
        dx = x2 - x1
        dy = y2 - y1
        normal = np.array([-dy, dx]) / np.hypot(dx, dy)
        return normal
    def estimate(self, lines):
        A = []
        b = []
        for (x1, y1), (x2, y2) in lines:
            n = self._normal_vector(x1, y1, x2, y2)
            A.append(n)
            b.append(np.dot(n, [x1, y1]))
        A = np.array(A)
        b = np.array(b)
        vp = np.linalg.lstsq(A, b, rcond=None)[0]
        return vp
